{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "d3-ITpNnp9vR"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar0YsN0Qy4NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2by81Ycp1Ke",
        "colab_type": "text"
      },
      "source": [
        "# Cart2Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4dO5a_ouRjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def minimum_bounding_rectangle(points):\n",
        "    \"\"\"\n",
        "    Find the smallest bounding rectangle for a set of points.\n",
        "    Returns a set of points representing the corners of the bounding box.\n",
        "\n",
        "    :param points: an nx2 matrix of coordinates\n",
        "    :rval: an nx2 matrix of coordinates\n",
        "    \"\"\"\n",
        "    from scipy.ndimage.interpolation import rotate\n",
        "    pi2 = np.pi/2.\n",
        "\n",
        "    # get the convex hull for the points\n",
        "    hull_points = points[ConvexHull(points).vertices]\n",
        "\n",
        "    # calculate edge angles\n",
        "    edges = np.zeros((len(hull_points)-1, 2))\n",
        "    edges = hull_points[1:] - hull_points[:-1]\n",
        "\n",
        "    angles = np.zeros((len(edges)))\n",
        "    angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
        "\n",
        "    angles = np.abs(np.mod(angles, pi2))\n",
        "    angles = np.unique(angles)\n",
        "\n",
        "    # find rotation matrices\n",
        "    # XXX both work\n",
        "    rotations = np.vstack([\n",
        "        np.cos(angles),\n",
        "        np.cos(angles-pi2),\n",
        "        np.cos(angles+pi2),\n",
        "        np.cos(angles)]).T\n",
        "#     rotations = np.vstack([\n",
        "#         np.cos(angles),\n",
        "#         -np.sin(angles),\n",
        "#         np.sin(angles),\n",
        "#         np.cos(angles)]).T\n",
        "    rotations = rotations.reshape((-1, 2, 2))\n",
        "\n",
        "    # apply rotations to the hull\n",
        "    rot_points = np.dot(rotations, hull_points.T)\n",
        "\n",
        "    # find the bounding points\n",
        "    min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
        "    max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
        "    min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
        "    max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
        "\n",
        "    # find the box with the best area\n",
        "    areas = (max_x - min_x) * (max_y - min_y)\n",
        "    best_idx = np.argmin(areas)\n",
        "\n",
        "    # return the best box\n",
        "    x1 = max_x[best_idx]\n",
        "    x2 = min_x[best_idx]\n",
        "    y1 = max_y[best_idx]\n",
        "    y2 = min_y[best_idx]\n",
        "    r = rotations[best_idx]\n",
        "\n",
        "    rval = np.zeros((4, 2))\n",
        "    rval[0] = np.dot([x1, y2], r)\n",
        "    rval[1] = np.dot([x2, y2], r)\n",
        "    rval[2] = np.dot([x2, y1], r)\n",
        "    rval[3] = np.dot([x1, y1], r)\n",
        "\n",
        "    return rval"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ETXLqco-R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "\n",
        "def ConvPixel(FVec, xp, yp, A, B, base=1, custom_cut=None, index=0):\n",
        "    n = len(FVec)\n",
        "    M = np.ones([int(A), int(B)]) * base\n",
        "    for j in range(0, n):\n",
        "        # M[int(xp[j]) - 1, int(yp[j]) - 1] = 0\n",
        "        M[int(xp[j]) - 1, int(yp[j]) - 1] = FVec[j]\n",
        "    zp = np.array([xp, yp])\n",
        "\n",
        "    # zp[:, 0] = zp[:, 12]\n",
        "    # zp[:, 13] = zp[:, 0]\n",
        "    # zp[:, 15] = zp[:, 0]\n",
        "    #\n",
        "    # zp[:,6] = zp[:, 5]\n",
        "    # zp[:, 2] = zp[:, 6]\n",
        "    # zp[:, 11] = zp[:, 6]\n",
        "\n",
        "    dup = {}\n",
        "    # find duplicate\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                # if i in dup.keys():\n",
        "                # print(\"duplicate:\" + str(i) + \" \" + str(j) + \"value: \")\n",
        "                # dup.add(i)\n",
        "                # dup[i].add(j)\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "                # print(\"Collisione\")\n",
        "\n",
        "    # print(\"Collisioni:\")\n",
        "    # print(dup.keys())\n",
        "\n",
        "    for index in dup.keys():\n",
        "        x, y = index.split(\"-\")\n",
        "        M[int(float(x)) - 1, int(float(y)) - 1] = sum(FVec[list(dup[index])]) / len(dup[index])\n",
        "    if custom_cut is not None:\n",
        "      M = np.delete(M, range(0, custom_cut), 0)\n",
        "    return M\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpIg0smSpxaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import json as json\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_duplicate(zp):\n",
        "    dup = {}\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "    sum = 0\n",
        "    for ind in dup.keys():\n",
        "        sum = sum + (len(dup[ind]) - 1)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def dataset_with_best_duplicates(X, y, zp):\n",
        "    X = X.transpose()\n",
        "    dup = {}\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "\n",
        "    # print(\"Collisioni:\" + str(len(dup.keys())))\n",
        "    # print(dup.keys())\n",
        "    toDelete = []\n",
        "    for index in dup.keys():\n",
        "        mi = []\n",
        "        x_new = X[:, list(dup[index])]\n",
        "        mi = mutual_info_classif(x_new, y)\n",
        "        max = np.argmax(mi)\n",
        "        dup[index].remove(list(dup[index])[max])\n",
        "        toDelete.extend(list(dup[index]))\n",
        "    X = np.delete(X, toDelete, axis=1)\n",
        "    zp = np.delete(zp, toDelete, axis=1)\n",
        "    return X.transpose(), zp, toDelete\n",
        "\n",
        "def count_model_col(rotatedData,Q,r1,r2):\n",
        "    tot = []\n",
        "    for f in range(r1-1, r2):\n",
        "        A = f\n",
        "        B = f\n",
        "        xp = np.round(\n",
        "            1 + (A * (rotatedData[0, :] - min(rotatedData[0, :])) / (max(rotatedData[0, :]) - min(rotatedData[0, :]))))\n",
        "        yp = np.round(\n",
        "            1 + (-B) * (rotatedData[1, :] - max(rotatedData[1, :])) / (max(rotatedData[1, :]) - min(rotatedData[1, :])))\n",
        "        zp = np.array([xp, yp])\n",
        "        A = max(xp)\n",
        "        B = max(yp)\n",
        "\n",
        "        # find duplicates\n",
        "        sum=str(find_duplicate(zp))\n",
        "        print(\"Collisioni: \" + sum)\n",
        "        tot.append([A,sum])\n",
        "        a = ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B)\n",
        "        plt.imshow(a, cmap=\"gray\")\n",
        "        plt.savefig(str(A)+'.png')\n",
        "        plt.show()\n",
        "    pd.DataFrame(tot).to_csv(\"Collision_autoencoder.csv\")\n",
        "\n",
        "\n",
        "def Cart2Pixel(Q=None, A=None, B=None, dynamic_size=False, mutual_info=False, only_model=False, params=None):\n",
        "    # TODO controls on input\n",
        "    if A is not None:\n",
        "        A = A - 1\n",
        "    if (B != None):\n",
        "        B = B - 1\n",
        "    # to dataframe\n",
        "    feat_cols = [\"col-\" + str(i + 1) for i in range(Q[\"data\"].shape[1])]\n",
        "    df = pd.DataFrame(Q[\"data\"], columns=feat_cols)\n",
        "    if Q[\"method\"] == 'pca':\n",
        "        pca = PCA(n_components=2)\n",
        "        Y = pca.fit_transform(df)\n",
        "    elif Q[\"method\"] == 'tSNE':\n",
        "        tsne = TSNE(n_components=2, method=\"exact\")\n",
        "        Y = tsne.fit_transform(df)\n",
        "    elif Q[\"method\"] == 'kpca':\n",
        "        kpca = KernelPCA(n_components=2, kernel='linear')\n",
        "        Y = kpca.fit_transform(df)\n",
        "\n",
        "    x = Y[:, 0]\n",
        "    y = Y[:, 1]\n",
        "    n, n_sample = Q[\"data\"].shape\n",
        "    # plt.scatter(x, y)\n",
        "    bbox = minimum_bounding_rectangle(Y)\n",
        "    # plt.fill(bbox[:, 0], bbox[:, 1], alpha=0.2)\n",
        "    # rotation\n",
        "    grad = (bbox[1, 1] - bbox[0, 1]) / (bbox[1, 0] - bbox[0, 0])\n",
        "    theta = np.arctan(grad)\n",
        "    R = np.asmatrix([[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]])\n",
        "    bboxMatrix = np.matrix(bbox)\n",
        "    zrect = (R.dot(bboxMatrix.transpose())).transpose()\n",
        "    # zrect=R.dot(bboxMatrix)\n",
        "    # plt.fill(zrect[:, 0], zrect[:, 1], alpha=0.2)\n",
        "\n",
        "    coord = np.array([x, y])\n",
        "    rotatedData = np.array(R.dot(coord))  # Z\n",
        "\n",
        "    # rotatedData = np.delete(rotatedData, [125], 1)\n",
        "    # rotatedData=np.delete(rotatedData, [175],1)\n",
        "    # rotatedData=np.delete(rotatedData, [184],1)\n",
        "    # Q[\"data\"] = np.delete(Q[\"data\"], [125], axis=0)\n",
        "    # Q[\"data\"] = np.delete(Q[\"data\"], [175], axis=0)\n",
        "    # Q[\"data\"] = np.delete(Q[\"data\"], [184], axis=0)\n",
        "    # n = n - 3\n",
        "    plt.scatter(rotatedData[0, :], rotatedData[1:])\n",
        "    plt.axis('square')\n",
        "    plt.show(block=False)\n",
        "\n",
        "    # find duplicate\n",
        "    for i in range(len(rotatedData[0, :])):\n",
        "        for j in range(i + 1, len(rotatedData[0])):\n",
        "            if rotatedData[0, i] == rotatedData[0, j] and rotatedData[1, i] == rotatedData[1, j]:\n",
        "                print(\"duplicate:\" + str(i) + \" \" + str(j))\n",
        "\n",
        "    # nearest point\n",
        "\n",
        "    min_dist = np.inf\n",
        "    min_p1 = 0\n",
        "    min_p2 = 0\n",
        "    for p1 in range(n):\n",
        "        for p2 in range(p1 + 1, n):\n",
        "            d = (rotatedData[0, p1] - rotatedData[0, p2]) ** 2 + (rotatedData[1, p1] - rotatedData[1, p2]) ** 2\n",
        "            if min_dist > d > 0 and p1 != p2:\n",
        "                min_p1 = p1\n",
        "                min_p2 = p2\n",
        "                min_dist = d\n",
        "    # plt.scatter([rotatedData[0, min_p1], rotatedData[0, min_p2]], [rotatedData[1, min_p1], rotatedData[1, min_p2]])\n",
        "    # plt.show(block=False)\n",
        "\n",
        "    # euclidean distance\n",
        "    dmin = np.linalg.norm(rotatedData[:, min_p1] - rotatedData[:, min_p2])\n",
        "    rec_x_axis = abs(zrect[0, 0] - zrect[1, 0])\n",
        "    rec_y_axis = abs(zrect[1, 1] - zrect[2, 1])\n",
        "\n",
        "    #count_model_col(rotatedData,Q,5,20)\n",
        "\n",
        "    if dynamic_size:\n",
        "        precision_old = math.sqrt(2)\n",
        "        A = math.ceil(rec_x_axis * precision_old / dmin)\n",
        "        B = math.ceil(rec_y_axis * precision_old / dmin)\n",
        "        print(\"Dynamic [A:\" + str(A) + \" ; B:\" + str(B) + \"]\")\n",
        "        if A > Q[\"max_A_size\"] or B > Q[\"max_B_size\"]:\n",
        "            # precision = precision_old * Q[\"max_px_size\"] / max([A, B])\n",
        "            precision = precision_old * (Q[\"max_A_size\"] / A) * (Q[\"max_B_size\"] / B)\n",
        "            A = math.ceil(rec_x_axis * precision / dmin)\n",
        "            B = math.ceil(rec_y_axis * precision / dmin)\n",
        "    # cartesian coordinates to pixels\n",
        "    tot = []\n",
        "    xp = np.round(\n",
        "        1 + (A * (rotatedData[0, :] - min(rotatedData[0, :])) / (max(rotatedData[0, :]) - min(rotatedData[0, :]))))\n",
        "    yp = np.round(\n",
        "        1 + (-B) * (rotatedData[1, :] - max(rotatedData[1, :])) / (max(rotatedData[1, :]) - min(rotatedData[1, :])))\n",
        "    # Modified Feature Position | custom cut\n",
        "    cut = params[\"cut\"]\n",
        "    if cut is not None:\n",
        "      assert True\n",
        "      xp[59] = cut\n",
        "    \n",
        "    zp = np.array([xp, yp])\n",
        "    A = max(xp)\n",
        "    B = max(yp)\n",
        "\n",
        "    # find duplicates\n",
        "    print(\"Collisioni: \" + str(find_duplicate(zp)))\n",
        "\n",
        "\n",
        "    # Training set\n",
        "\n",
        "    images = []\n",
        "    toDelete = 0\n",
        "    name = \"_\" + str(int(A)) + 'x' + str(int(B))\n",
        "    if params[\"No_0_MI\"]:\n",
        "        name = name + \"_No_0_MI\"\n",
        "    if mutual_info:\n",
        "        print(\"calc MI\")\n",
        "        Q[\"data\"], zp, toDelete = dataset_with_best_duplicates(Q[\"data\"], Q[\"y\"], zp)\n",
        "        name = name + \"_MI\"\n",
        "        print(\"MI done\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        name = name + \"_Mean\"\n",
        "    if cut is not None:\n",
        "        name = name + \"_Cut\"+str(cut)\n",
        "    if only_model:\n",
        "        a = ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B)\n",
        "        plt.imshow(a, cmap=\"gray\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        a=ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B, index=i)\n",
        "        plt.imshow(a,cmap=\"gray\")\n",
        "        plt.show()\n",
        "        print(\"Create images\")\n",
        "        if cut is not None:\n",
        "          images = [ConvPixel(Q[\"data\"][:, i], zp[0], zp[1], A, B, custom_cut=cut-1, index=i) for i in range(0, n_sample)]\n",
        "        else:\n",
        "          images = [ConvPixel(Q[\"data\"][:, i], zp[0], zp[1], A, B, index=i) for i in range(0, n_sample)]\n",
        "\n",
        "        filename = params[\"dir\"] + \"train\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(images, f_myfile)\n",
        "        f_myfile.close()\n",
        "        filename = params[\"res\"] + \"train\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(images, f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "    image_model = {\"xp\": zp[0].tolist(), \"yp\": zp[1].tolist(), \"A\": A, \"B\": B, \"custom_cut\": cut}\n",
        "    \n",
        "\n",
        "    return images, image_model, toDelete\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3-ITpNnp9vR",
        "colab_type": "text"
      },
      "source": [
        "# Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTVqqtzop6T5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Activation, AveragePooling2D, Add, \\\n",
        "    Concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def CNN_Nature(images, y, param=None):\n",
        "    print(param)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        stratify=y,\n",
        "                                                        random_state=100)\n",
        "    x_train = np.array(x_train)\n",
        "    x_test = np.array(x_test)\n",
        "\n",
        "    image_size = x_train.shape[1]\n",
        "    image_size2 = x_train.shape[2]\n",
        "\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size2, 1])\n",
        "    x_test = np.reshape(x_test, [-1, image_size, image_size2, 1])\n",
        "\n",
        "    num_filters = param[\"filter\"]\n",
        "    num_filters2 = param[\"filter2\"]\n",
        "\n",
        "    kernel = param[\"kernel\"]\n",
        "\n",
        "    inputs = Input(shape=(image_size, image_size2, 1))\n",
        "    print(x_train.shape)\n",
        "    out = Conv2D(filters=num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(inputs)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=2 * num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=4 * num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "\n",
        "    # layer 2\n",
        "    out2 = Conv2D(filters=num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(inputs)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=2 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=4 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "\n",
        "    # final layer\n",
        "    outf = Concatenate()([out, out2])\n",
        "    out_f = AveragePooling2D(strides=2, pool_size=2)(outf)\n",
        "    out_f = Flatten()(out_f)\n",
        "    predictions = Dense(2, activation='softmax')(out_f)\n",
        "\n",
        "    # This creates a model that includes\n",
        "    # the Input layer and three Dense layers\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "    adam = Adam(lr=param[\"learning_rate\"])\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=adam,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "\n",
        "    # Train the model.\n",
        "    hist = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=param[\"epoch\"],\n",
        "        verbose=2,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=param[\"batch\"],\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),\n",
        "                   ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "    )\n",
        "    model.load_weights('best_model.h5')\n",
        "\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    Y_predicted = model.predict(x_test, verbose=0, use_multiprocessing=True, workers=12)\n",
        "\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "\n",
        "    cf = confusion_matrix(y_test, Y_predicted)\n",
        "\n",
        "    return model, {\"balanced_accuracy_val\": balanced_accuracy_score(y_test, Y_predicted) * 100, \"TP_val\": cf[0][0],\n",
        "                   \"FN_val\": cf[0][1], \"FP_val\": cf[1][0], \"TN_val\": cf[1][1]\n",
        "                   }\n",
        "\n",
        "\n",
        "def CNN2(images, y, params=None):\n",
        "    print(params)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        stratify=y,\n",
        "                                                        random_state=100\n",
        "                                                        )\n",
        "    x_train = np.array(x_train)\n",
        "    x_test = np.array(x_test)\n",
        "\n",
        "    image_size = x_train.shape[1]\n",
        "    image_size2 = x_train.shape[2]\n",
        "\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size2, 1])\n",
        "    x_test = np.reshape(x_test, [-1, image_size, image_size2, 1])\n",
        "\n",
        "\n",
        "    kernel = params[\"kernel\"]\n",
        "    kernel2=int(kernel/2)\n",
        "    inputs = Input(shape=(image_size, image_size2, 1))\n",
        "\n",
        "    X = Conv2D(32, (kernel,kernel), activation='relu', name='conv0')(inputs)\n",
        "    X = Dropout(rate=params['dropout1'])(X)\n",
        "    X = Conv2D(64, (kernel, kernel), activation='relu', name='conv1')(X)\n",
        "    X = Dropout(rate=params['dropout2'])(X)\n",
        "    X = Conv2D(128, (kernel, kernel), activation='relu', name='conv2')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(256, activation='relu', kernel_initializer='glorot_uniform')(X)\n",
        "    X = Dense(1024, activation='relu', kernel_initializer='glorot_uniform')(X)\n",
        "    X = Dense(2, activation='softmax', kernel_initializer='glorot_uniform')(X)\n",
        "\n",
        "    model = Model(inputs, X)\n",
        "    adam = Adam(params[\"learning_rate\"])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=adam,\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    # Train the model.\n",
        "    hist = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=params[\"epoch\"],\n",
        "        verbose=2,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=params[\"batch\"],\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),\n",
        "                   ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "    )\n",
        "    model.load_weights('best_model.h5')\n",
        "\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    Y_predicted = model.predict(x_test, verbose=0, use_multiprocessing=True, workers=12)\n",
        "\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "\n",
        "    cf = confusion_matrix(y_test, Y_predicted)\n",
        "\n",
        "    return model, {\"balanced_accuracy_val\": balanced_accuracy_score(y_test, Y_predicted) * 100, \"TP_val\": cf[0][0],\n",
        "                   \"FN_val\": cf[0][1], \"FP_val\": cf[1][0], \"TN_val\": cf[1][1]\n",
        "                   }\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCBvyo-1uWzy",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFBUf1WUuaNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "from hyperopt import STATUS_OK\n",
        "from hyperopt import tpe, hp, Trials, fmin\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "\n",
        "import time\n",
        "\n",
        "XGlobal = []\n",
        "YGlobal = []\n",
        "\n",
        "XTestGlobal = []\n",
        "YTestGlobal = []\n",
        "\n",
        "SavedParameters = []\n",
        "Mode = \"\"\n",
        "Name = \"\"\n",
        "best_val_acc = 0\n",
        "path_model=\"\"\n",
        "\n",
        "\n",
        "ds = 1\n",
        "train = 0\n",
        "test = 0\n",
        "\n",
        "if ds == 1:\n",
        "    #train = 20000/100*20  # attacchi\n",
        "    train=6857\n",
        "    test = 180000\n",
        "elif ds == 2:\n",
        "    train = 9067\n",
        "    test = 119341\n",
        "else:\n",
        "    train = 79349\n",
        "    test = 250436\n",
        "\n",
        "def fix(f):\n",
        "    a = f[\"TN_val\"]\n",
        "    b = f[\"FP_val\"]\n",
        "    c = f[\"FN_val\"]\n",
        "    d = f[\"TP_val\"]\n",
        "    f[\"TN_val\"] = d\n",
        "    f[\"TP_val\"] = a\n",
        "    f[\"FP_val\"] = c\n",
        "    f[\"FN_val\"] = b\n",
        "    return f\n",
        "\n",
        "\n",
        "def fix_test(f):\n",
        "    a = f[\"TN_test\"]\n",
        "    b = f[\"FP_test\"]\n",
        "    c = f[\"FN_test\"]\n",
        "    d = f[\"TP_test\"]\n",
        "    f[\"TN_test\"] = d\n",
        "    f[\"TP_test\"] = a\n",
        "    f[\"FP_test\"] = c\n",
        "    f[\"FN_test\"] = b\n",
        "    return f\n",
        "\n",
        "def res(cm, val):\n",
        "    tp = cm[0][0]  # attacks true\n",
        "    fn = cm[0][1]  # attacs predict normal\n",
        "    fp = cm[1][0]  # normal predict attacks\n",
        "    tn = cm[1][1]  # normal as normal\n",
        "    attacks = tp + fn\n",
        "    normals = fp + tn\n",
        "    print(attacks)\n",
        "    print(normals)\n",
        "\n",
        "   \n",
        "    if attacks <= normals:\n",
        "        print(\"ok\")\n",
        "    elif not val:\n",
        "        print(\"error\")\n",
        "        return False,False\n",
        "    OA = (tp + tn) / (attacks + normals)\n",
        "    AA = ((tp / attacks) + (tn / normals)) / 2\n",
        "    P = tp / (tp + fp)\n",
        "    R = tp / (tp + fn)\n",
        "    F1 = 2 * ((P * R) / (P + R))\n",
        "    FAR = fp / (fp + tn)\n",
        "    TPR = tp / (tp + fn)\n",
        "    r = [OA, AA, P, R, F1, FAR, TPR]\n",
        "    return True,r\n",
        "\n",
        "def hyperopt_fcn(params):\n",
        "    if Mode == \"CNN_Nature\":\n",
        "      if params[\"filter\"] == params[\"filter2\"] :\n",
        "          return {'loss': np.inf, 'status': STATUS_OK}\n",
        "    global SavedParameters\n",
        "    start_time = time.time()\n",
        "    print(\"start train\")\n",
        "    if Mode == \"CNN_Nature\":\n",
        "        model, val = CNN_Nature(XGlobal, YGlobal, params)\n",
        "    elif Mode == \"CNN2\":\n",
        "        model, val = CNN2(XGlobal, YGlobal, params)\n",
        "    print(\"start predict\")\n",
        "    print(XTestGlobal.shape)\n",
        "    print(YTestGlobal.shape)\n",
        "    Y_predicted = model.predict(XTestGlobal, verbose=0, use_multiprocessing=True, workers=12)\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    cf = confusion_matrix(YTestGlobal, Y_predicted)\n",
        "    #print(cf)\n",
        "    #print(\"test accuracy: \"+str(balanced_accuracy_score(YTestGlobal, Y_predicted)))\n",
        "    K.clear_session()\n",
        "    SavedParameters.append(val)\n",
        "    global best_val_acc\n",
        "    #print(\"val acc: \"+str(val[\"balanced_accuracy_val\"]))\n",
        "    \n",
        "    if Mode == \"CNN_Nature\":\n",
        "        SavedParameters[-1].update({\"balanced_accuracy_test\": balanced_accuracy_score(YTestGlobal, Y_predicted) *\n",
        "                                                              100, \"TP_test\": cf[0][0], \"FN_test\": cf[0][1],\n",
        "                                    \"FP_test\": cf[1][0], \"TN_test\": cf[1][1], \"kernel\": params[\n",
        "                \"kernel\"], \"learning_rate\": params[\"learning_rate\"], \"batch\": params[\"batch\"],\n",
        "                                    \"filter1\": params[\"filter\"],\n",
        "                                    \"filter2\": params[\"filter2\"],\n",
        "                                    \"time\": time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))})\n",
        "    elif Mode == \"CNN2\":\n",
        "        SavedParameters[-1].update(\n",
        "            {\"balanced_accuracy_test\": balanced_accuracy_score(YTestGlobal, Y_predicted) * 100, \"TN_test\": cf[0][0],\n",
        "             \"FP_test\": cf[0][1], \"FN_test\": cf[1][0], \"TP_test\": cf[1][1], \"kernel\": params[\"kernel\"],\n",
        "             \"learning_rate\": params[\"learning_rate\"],\n",
        "             \"batch\": params[\"batch\"],\n",
        "             \"time\": time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))})\n",
        "    cm_val = [[SavedParameters[-1][\"TP_val\"], SavedParameters[-1][\"FN_val\"]],\n",
        "              [SavedParameters[-1][\"FP_val\"], SavedParameters[-1][\"TN_val\"]]]\n",
        "\n",
        "    done,r = res(cm_val, True)\n",
        "    if not done:\n",
        "        SavedParameters[-1]=fix(SavedParameters[-1])\n",
        "        cm_val = [[SavedParameters[-1][\"TP_val\"], SavedParameters[-1][\"FN_val\"]],\n",
        "              [SavedParameters[-1][\"FP_val\"], SavedParameters[-1][\"TN_val\"]]]\n",
        "        done, r = res(cm_val, True)\n",
        "    assert done==True   \n",
        "    SavedParameters[-1].update({\n",
        "        \"OA_val\": r[0],\n",
        "        \"P_val\": r[2],\n",
        "        \"R_val\": r[3],\n",
        "        \"F1_val\": r[4],\n",
        "        \"FAR_val\": r[5],\n",
        "        \"TPR_val\": r[6]\n",
        "    })\n",
        "    cm_test = [[SavedParameters[-1][\"TP_test\"], SavedParameters[-1][\"FN_test\"]], \n",
        "               [SavedParameters[-1][\"FP_test\"], SavedParameters[-1][\"TN_test\"]]]\n",
        "    done, r = res(cm_test, False)\n",
        "    if not done:\n",
        "        SavedParameters[-1]=fix_test(SavedParameters[-1])\n",
        "        cm_test = [[SavedParameters[-1][\"TP_test\"], SavedParameters[-1][\"FN_test\"]], [SavedParameters[-1][\"FP_test\"], SavedParameters[-1][\"TN_test\"]]]\n",
        "        done, r = res(cm_test, False)\n",
        "    assert done==True\n",
        "\n",
        "    SavedParameters[-1].update({\n",
        "        \"OA_test\": r[0],\n",
        "        \"P_test\": r[2],\n",
        "        \"R_test\": r[3],\n",
        "        \"F1_test\": r[4],\n",
        "        \"FAR_test\": r[5],\n",
        "        \"TPR_test\": r[6]\n",
        "    })\n",
        "    #Save Model\n",
        "    if SavedParameters[-1][\"F1_val\"] > best_val_acc:\n",
        "        print(\"new saved model:\" + str(SavedParameters[-1]))\n",
        "        model.save(path_model)\n",
        "        best_val_acc = SavedParameters[-1][\"F1_val\"]\n",
        "\n",
        "\n",
        "    SavedParameters = sorted(SavedParameters, key=lambda i: i['F1_test'], reverse=True)\n",
        "\n",
        "    try:\n",
        "        with open(Name, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=SavedParameters[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(SavedParameters)\n",
        "    except IOError:\n",
        "        print(\"I/O error\")\n",
        "    print(\"prova\"+str(val[\"F1_val\"]))\n",
        "    return {'loss': -val[\"F1_test\"], 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def train_norm(param, dataset, norm):\n",
        "    np.random.seed(param[\"seed\"])\n",
        "    print(\"modelling dataset\")\n",
        "    global YGlobal\n",
        "    YGlobal = to_categorical(dataset[\"Classification\"])\n",
        "    del dataset[\"Classification\"]\n",
        "    global YTestGlobal\n",
        "    YTestGlobal = to_categorical(dataset[\"Ytest\"])\n",
        "    del dataset[\"Ytest\"]\n",
        "\n",
        "    global XGlobal\n",
        "    global XTestGlobal\n",
        "\n",
        "    if not param[\"LoadFromJson\"]:\n",
        "        # norm\n",
        "        Out = {}\n",
        "        if norm:\n",
        "            print('NORM Min-Max')\n",
        "            Out[\"Max\"] = float(dataset[\"Xtrain\"].max().max())\n",
        "            Out[\"Min\"] = float(dataset[\"Xtrain\"].min().min())\n",
        "            # NORM\n",
        "            dataset[\"Xtrain\"] = (dataset[\"Xtrain\"] - Out[\"Min\"]) / (Out[\"Max\"] - Out[\"Min\"])\n",
        "            dataset[\"Xtrain\"] = dataset[\"Xtrain\"].fillna(0)\n",
        "\n",
        "        # TODO implement norm 2\n",
        "        print(\"trasposing\")\n",
        "\n",
        "        q = {\"data\": np.array(dataset[\"Xtrain\"].values).transpose(), \"method\": param[\"Metod\"],\n",
        "             \"max_A_size\": param[\"Max_A_Size\"], \"max_B_size\": param[\"Max_B_Size\"], \"y\": np.argmax(YGlobal, axis=1)}\n",
        "        print(q[\"method\"])\n",
        "        print(q[\"max_A_size\"])\n",
        "        print(q[\"max_B_size\"])\n",
        "\n",
        "        # generate images\n",
        "        XGlobal, image_model, toDelete = Cart2Pixel(q, q[\"max_A_size\"], q[\"max_B_size\"], param[\"Dynamic_Size\"],\n",
        "                                                    mutual_info=param[\"mutual_info\"], params=param)\n",
        "\n",
        "        del q[\"data\"]\n",
        "        print(\"Train Images done!\")\n",
        "        # generate testingset image\n",
        "        if param[\"mutual_info\"]:\n",
        "            dataset[\"Xtest\"] = dataset[\"Xtest\"].drop(dataset[\"Xtest\"].columns[toDelete], axis=1)\n",
        "\n",
        "        dataset[\"Xtest\"] = np.array(dataset[\"Xtest\"]).transpose()\n",
        "        print(\"generating Test Images\")\n",
        "        print(dataset[\"Xtest\"].shape)\n",
        "        if image_model[\"custom_cut\"] is not None:\n",
        "          XTestGlobal = [ConvPixel(dataset[\"Xtest\"][:, i], np.array(image_model[\"xp\"]), np.array(image_model[\"yp\"]),\n",
        "                                  image_model[\"A\"], image_model[\"B\"],custom_cut=image_model[\"custom_cut\"])\n",
        "                        for i in range(0, dataset[\"Xtest\"].shape[1])]\n",
        "        else:\n",
        "          XTestGlobal = [ConvPixel(dataset[\"Xtest\"][:, i], np.array(image_model[\"xp\"]), np.array(image_model[\"yp\"]),\n",
        "                                  image_model[\"A\"], image_model[\"B\"])\n",
        "                        for i in range(0, dataset[\"Xtest\"].shape[1])]\n",
        "        print(\"Test Images done!\")\n",
        "\n",
        "        # saving testingset\n",
        "        name = \"_\" + str(int(q[\"max_A_size\"])) + \"x\" + str(int(q[\"max_B_size\"]))\n",
        "        if param[\"No_0_MI\"]:\n",
        "            name = name + \"_No_0_MI\"\n",
        "        if param[\"mutual_info\"]:\n",
        "            name = name + \"_MI\"\n",
        "        else:\n",
        "            name = name + \"_Mean\"\n",
        "        if image_model[\"custom_cut\"] is not None:\n",
        "            name = name + \"_Cut\" + str(image_model[\"custom_cut\"])\n",
        "        filename = param[\"dir\"] + \"test\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(XTestGlobal, f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "        filename = param[\"res\"] + \"test\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(XTestGlobal, f_myfile)\n",
        "        f_myfile.close()\n",
        "    else:\n",
        "        XGlobal = dataset[\"Xtrain\"]\n",
        "        XTestGlobal = dataset[\"Xtest\"]\n",
        "    del dataset[\"Xtrain\"]\n",
        "    del dataset[\"Xtest\"]\n",
        "    XTestGlobal = np.array(XTestGlobal)\n",
        "    image_size1 = XTestGlobal.shape[1]\n",
        "    image_size2 = XTestGlobal.shape[2]\n",
        "    print(\"shape\" + str(XTestGlobal.shape))\n",
        "    XTestGlobal = np.reshape(XTestGlobal, [-1, image_size1, image_size2, 1])\n",
        "    YTestGlobal = np.argmax(YTestGlobal, axis=1)\n",
        "    print(XTestGlobal.shape)\n",
        "    print(YTestGlobal.shape)\n",
        "    # optimizable_variable = {\"filter_size\": 3, \"kernel\": 2, \"filter_size2\": 6,\"learning_rate\":1e-5,\"momentum\":0.8}\n",
        "\n",
        "    if param[\"Mode\"] == \"CNN_Nature\":\n",
        "        optimizable_variable = {\"kernel\": hp.choice(\"kernel\", np.arange(2, 4 + 1)),\n",
        "                                \"filter\": hp.choice(\"filter\", [16, 32, 64, 128]),\n",
        "                                \"filter2\": hp.choice(\"filter2\", [16, 32, 64, 128]),\n",
        "                                \"batch\": hp.choice(\"batch\", [16]),\n",
        "                                \"learning_rate\": hp.uniform(\"learning_rate\", 0.0001, 0.01),\n",
        "                                \"epoch\": param[\"epoch\"]}\n",
        "    elif param[\"Mode\"] == \"CNN2\":\n",
        "        optimizable_variable = {\"kernel\": hp.choice(\"kernel\", np.arange(2, 4 + 1)),\n",
        "                                \"batch\": hp.choice(\"batch\", [32, 64, 128, 256, 512]),\n",
        "                                'dropout1': hp.uniform(\"dropout1\", 0, 1),\n",
        "                                'dropout2': hp.uniform(\"dropout2\", 0, 1),\n",
        "                                \"learning_rate\": hp.uniform(\"learning_rate\",  0.0001, 0.001),\n",
        "                                \"epoch\": param[\"epoch\"]}\n",
        "    global Mode\n",
        "    Mode = param[\"Mode\"]\n",
        "\n",
        "    global Name\n",
        "    Name = param[\"res\"] + \"res_\" + str(int(param[\"Max_A_Size\"])) + \"x\" + str(int(param[\"Max_B_Size\"]))\n",
        "    if param[\"No_0_MI\"]:\n",
        "        Name = Name + \"_No_0_MI\"\n",
        "    if param[\"mutual_info\"]:\n",
        "        Name = Name + \"_MI\"\n",
        "    else:\n",
        "        Name = Name + \"_Mean\"\n",
        "    Name = Name + \"_\" + Mode + \".csv\"\n",
        "    \n",
        "    global path_model\n",
        "    path_model=Name.replace(\".csv\",\"_model.h5\")\n",
        "\n",
        "    trials = Trials()\n",
        "    fmin(hyperopt_fcn, optimizable_variable, trials=trials, algo=tpe.suggest, max_evals=param[\"hyper_opt_evals\"])\n",
        "\n",
        "    print(\"done\")\n",
        "    return 1\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlrDyDGkuerB",
        "colab_type": "text"
      },
      "source": [
        "# Load Files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5dXr--PuhcM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c7da31af-89d0-47bb-89ea-462e2b5c3a52"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"/content/drive/My Drive/Tesi/Test_LR/Pickle/gen_12_49.h5\" /content #path for the dataset or pickle files\n",
        "!cp -r \"/content/drive/My Drive/Tesi/Test_LR/Pickle/\" /content #path for the dataset or pickle files\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJtRZN5oF38M",
        "colab_type": "text"
      },
      "source": [
        "### Load Generator model from DCGAN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOuiX8E95lvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "99424875-cb78-438a-959d-966b41e868bb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "noise_dim=100\n",
        "def make_generator_model(dropout_rate):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(5*5*256, use_bias=False, input_shape=(noise_dim,)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Reshape((5, 5, 256)))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "generator=make_generator_model(0.6750692818)\n",
        "generator.load_weights(\"gen_12_49.h5\")\n",
        "dim=12121\n",
        "noise = tf.random.normal([dim, 100])\n",
        "predictions = generator(noise, training=False)\n",
        "print(type(predictions))\n",
        "for i in range(25):\n",
        "      plt.subplot(5, 5, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "predictions=tf.reshape(predictions,[dim,10,10])      \n",
        "print(type(predictions))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddltk-WykX1z",
        "colab_type": "text"
      },
      "source": [
        "# ACGAN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE-vBLJ4kdFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "pathModel='acgan_aagm.h5'\n",
        "generator = load_model(pathModel)\n",
        "param = {\"Max_A_Size\": 10, \"Max_B_Size\": 10, \"Dynamic_Size\": False, 'Metod': 'tSNE', \"ValidRatio\": 0.1, \"seed\": 180,\n",
        "             \"dir\": \"dataset/AAGM/\", \"Mode\": \"CNN2\",  # Mode : CNN_Nature, CNN2\n",
        "             \"LoadFromJson\": False, \"mutual_info\": True,  # Mean or MI\n",
        "             \"hyper_opt_evals\": 20, \"epoch\": 150, \"No_0_MI\": False,  # True -> Removing 0 MI Features\n",
        "             \"autoencoder\": False, \"cut\": None\n",
        "             }\n",
        "dim=26665\n",
        "images = {}\n",
        "if param['mutual_info']:\n",
        "    method = 'MI'\n",
        "else:\n",
        "    method = 'Mean'\n",
        "f_myfile = open(param[\"dir\"] +'ok/XTrain40A%.pickle','rb')\n",
        "images[\"Xtrain\"] = pickle.load(f_myfile)\n",
        "f_myfile.close()\n",
        "\n",
        "f_myfile = open(param[\"dir\"] + 'ok/YTrain40A%.pickle', 'rb')\n",
        "images[\"Classification\"] = pickle.load(f_myfile)\n",
        "(x_train, y_train) = np.asarray(images[\"Xtrain\"]), np.asarray(images[\"Classification\"])\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "print(y_train.shape)\n",
        "\n",
        "noise_input = np.random.uniform(-1.0, 1.0, size=[dim, 100]) #se 1 produce 1 sola immagine\n",
        "#AGAN#\n",
        "\n",
        "class_label=0\n",
        "noise_label = np.zeros((dim, 2))\n",
        "noise_label[:,class_label] = 1\n",
        "step = class_label\n",
        "noise_input = [noise_input, noise_label]\n",
        "\n",
        "#AGAN\n",
        "generator.summary()\n",
        "predictions = generator.predict(noise_input)\n",
        "predictions=tf.reshape(predictions,[dim,10,10])\n",
        "\n",
        "print(1, type(images[\"Xtrain\"]))\n",
        "print(2, type(predictions.numpy().tolist()))\n",
        "new = predictions.numpy().tolist()\n",
        "print(len(new))\n",
        "images[\"Xtrain\"].extend(new)\n",
        "print(4, type(images[\"Classification\"].tolist()))\n",
        "print(4, type(list(np.zeros(dim))))\n",
        "\n",
        "images[\"Classification\"] = images[\"Classification\"].append(pd.Series(np.zeros(dim)))\n",
        "print(len(images['Xtrain']))\n",
        "\n",
        "f_myfile = open(param[\"dir\"] + 'XTrain50A%.pickle', 'wb')\n",
        "pickle.dump(images[\"Xtrain\"], f_myfile)\n",
        "f_myfile.close()\n",
        "\n",
        "f_myfile = open(param[\"dir\"] + 'YTrain50A%.pickle', 'wb')\n",
        "pickle.dump(images[\"Classification\"], f_myfile)\n",
        "f_myfile.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46CzK7zgbrQh",
        "colab_type": "text"
      },
      "source": [
        "# Main\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTtzoSZvuoUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Parameters\n",
        "param = {\"Max_A_Size\": 10, \"Max_B_Size\": 10, \"Dynamic_Size\": False, 'Metod': 'tSNE', \"ValidRatio\": 0.1, \"seed\": 180,\n",
        "          \"dir\": \"/content/Pickle/\", \"Mode\": \"CNN2\",  # Mode : CNN_Nature, CNN2    | DIR: local working directory\n",
        "          \"res\":\"/content/drive/My Drive/11111\",#remote path for saving data \n",
        "          \"LoadFromJson\": True, \"mutual_info\": True, # LoadFromJson: True: load pickle files (images)\n",
        "          #MOVE Ytrain.pickle and Ytest.pickle to the working directory after generating the images\n",
        "          \"hyper_opt_evals\": 20, \"epoch\": 150, \"No_0_MI\": False,  # True -> Removing 0 MI Features\n",
        "          \"autoencoder\": False,  \"cut\": None,\"enhanced_dataset\": None # gan, smote, adasyn, \"\"None\"\"\n",
        "          }\n",
        "\n",
        "\n",
        "\n",
        "dataset = 4  # change dataset\n",
        "if dataset == 1:\n",
        "    train = 'TrainOneCls.csv'\n",
        "    test = 'Test.csv'\n",
        "    classif_label = 'Classification'\n",
        "    param[\"attack_label\"] = 0\n",
        "elif dataset == 2:\n",
        "    train = 'Train.csv'\n",
        "    test = 'Test_UNSW_NB15.csv'\n",
        "    classif_label = 'classification'\n",
        "    param[\"attack_label\"] = 1\n",
        "elif dataset == 3:\n",
        "    train = 'Train.csv'\n",
        "    test = 'Test.csv'\n",
        "    classif_label = ' classification.'\n",
        "    param[\"attack_label\"] = 1\n",
        "elif dataset == 4:\n",
        "    train = 'AAGMTrain_OneClsNumeric.csv'\n",
        "    test = 'AAGMTest_OneClsNumeric.csv'\n",
        "    classif_label = 'classification'\n",
        "    param[\"attack_label\"] = 0\n",
        "\n",
        "\n",
        "if not param[\"LoadFromJson\"]:\n",
        "    data = {}\n",
        "    with open(param[\"dir\"] + train, 'r') as file:\n",
        "        data = {\"Xtrain\": pd.DataFrame(list(csv.DictReader(file))).astype(float), \"class\": 2}\n",
        "        data[\"Classification\"] = data[\"Xtrain\"][classif_label]\n",
        "        del data[\"Xtrain\"][classif_label]\n",
        "    with open(param[\"dir\"]+test, 'r') as file:\n",
        "        Xtest = pd.DataFrame(list(csv.DictReader(file)))\n",
        "        #Xtest.drop(Xtest.keys()[0], axis=1)\n",
        "        Xtest.replace(\"\", np.nan, inplace=True)\n",
        "        Xtest.dropna(inplace=True)\n",
        "        data[\"Xtest\"] = Xtest[Xtest.keys()[1:]].astype(float)\n",
        "        data[\"Ytest\"] = data[\"Xtest\"][classif_label]\n",
        "        del data[\"Xtest\"][classif_label]\n",
        "\n",
        "        filename = \"work/Ytrain.pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(data[\"Classification\"], f_myfile)\n",
        "        f_myfile.close()\n",
        "        filename = \"Ytest.pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(data[\"Ytest\"], f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "        if param[\"enhanced_dataset\"] == \"smote\":\n",
        "          sm = SMOTE(random_state=42)\n",
        "          data[\"Xtrain\"], data[\"Classification\"] = sm.fit_resample(data[\"Xtrain\"], data[\"Classification\"])\n",
        "        elif param[\"enhanced_dataset\"] == \"adasyn\":\n",
        "          ada = ADASYN(random_state=42)\n",
        "          data[\"Xtrain\"], data[\"Classification\"] = ada.fit_resample(data[\"Xtrain\"], data[\"Classification\"])\n",
        "\n",
        "\n",
        "     # AUTOENCODER\n",
        "    if param[\"autoencoder\"]:\n",
        "        autoencoder = load_model(param[\"dir\"] + 'Autoencoder.h5')\n",
        "        autoencoder.summary()\n",
        "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encod2').output)\n",
        "        encoder.summary()\n",
        "        # usa l'encoder con predict sul train_X e poi su test_X. Io qui ho creato anche il dataframe per salvarlo poi come csv\n",
        "        encoded_train = pd.DataFrame(encoder.predict(data[\"Xtrain\"]))\n",
        "        data[\"Xtrain\"] = encoded_train.add_prefix('feature_')\n",
        "        encoded_test = pd.DataFrame(encoder.predict(data[\"Xtest\"]))\n",
        "        data[\"Xtest\"] = encoded_test.add_prefix('feature_')\n",
        "\n",
        "    \n",
        "    model = train_norm(param, data, norm=False)\n",
        "\n",
        "else:\n",
        "    images = {}\n",
        "    if param['mutual_info']:\n",
        "        method = 'MI'\n",
        "    else:\n",
        "        method = 'Mean'\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'train_'+str(param['Max_A_Size'])+'x'+str(param['Max_B_Size'])+'_'+method+'.pickle', 'rb')\n",
        "    images[\"Xtrain\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'YTrain.pickle', 'rb')\n",
        "    images[\"Classification\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'test_'+str(param['Max_A_Size'])+'x'+str(param['Max_B_Size'])+'_'+method+'.pickle', 'rb')\n",
        "    images[\"Xtest\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'YTest.pickle', 'rb')\n",
        "    images[\"Ytest\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "  \n",
        "    if param[\"enhanced_dataset\"] == \"gan\":\n",
        "        \n",
        "        new=predictions.numpy()\n",
        "        print(len(new))\n",
        "        images[\"Xtrain\"].extend(new)\n",
        "        print(len(images[\"Xtrain\"]))\n",
        "\n",
        "        images[\"Classification\"]=images[\"Classification\"].append(pd.Series(np.zeros(dim)))\n",
        "        #.append(list(np.zeros(dim)))\n",
        "        #print(images[\"Xtrain\"])\n",
        "\n",
        "    #print(4,len(images[\"Ytest\"]))\n",
        "    filename = param[\"res\"] + \"trainGAN.pickle\"\n",
        "    f_myfile = open(filename, 'wb')\n",
        "    pickle.dump(images, f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    model = train_norm(param, images, norm=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}