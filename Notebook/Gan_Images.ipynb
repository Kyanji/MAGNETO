{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CICDS_35.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "m2by81Ycp1Ke",
        "d3-ITpNnp9vR"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar0YsN0Qy4NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2by81Ycp1Ke",
        "colab_type": "text"
      },
      "source": [
        "# Cart2Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4dO5a_ouRjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def minimum_bounding_rectangle(points):\n",
        "    \"\"\"\n",
        "    Find the smallest bounding rectangle for a set of points.\n",
        "    Returns a set of points representing the corners of the bounding box.\n",
        "\n",
        "    :param points: an nx2 matrix of coordinates\n",
        "    :rval: an nx2 matrix of coordinates\n",
        "    \"\"\"\n",
        "    from scipy.ndimage.interpolation import rotate\n",
        "    pi2 = np.pi/2.\n",
        "\n",
        "    # get the convex hull for the points\n",
        "    hull_points = points[ConvexHull(points).vertices]\n",
        "\n",
        "    # calculate edge angles\n",
        "    edges = np.zeros((len(hull_points)-1, 2))\n",
        "    edges = hull_points[1:] - hull_points[:-1]\n",
        "\n",
        "    angles = np.zeros((len(edges)))\n",
        "    angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
        "\n",
        "    angles = np.abs(np.mod(angles, pi2))\n",
        "    angles = np.unique(angles)\n",
        "\n",
        "    # find rotation matrices\n",
        "    # XXX both work\n",
        "    rotations = np.vstack([\n",
        "        np.cos(angles),\n",
        "        np.cos(angles-pi2),\n",
        "        np.cos(angles+pi2),\n",
        "        np.cos(angles)]).T\n",
        "#     rotations = np.vstack([\n",
        "#         np.cos(angles),\n",
        "#         -np.sin(angles),\n",
        "#         np.sin(angles),\n",
        "#         np.cos(angles)]).T\n",
        "    rotations = rotations.reshape((-1, 2, 2))\n",
        "\n",
        "    # apply rotations to the hull\n",
        "    rot_points = np.dot(rotations, hull_points.T)\n",
        "\n",
        "    # find the bounding points\n",
        "    min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
        "    max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
        "    min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
        "    max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
        "\n",
        "    # find the box with the best area\n",
        "    areas = (max_x - min_x) * (max_y - min_y)\n",
        "    best_idx = np.argmin(areas)\n",
        "\n",
        "    # return the best box\n",
        "    x1 = max_x[best_idx]\n",
        "    x2 = min_x[best_idx]\n",
        "    y1 = max_y[best_idx]\n",
        "    y2 = min_y[best_idx]\n",
        "    r = rotations[best_idx]\n",
        "\n",
        "    rval = np.zeros((4, 2))\n",
        "    rval[0] = np.dot([x1, y2], r)\n",
        "    rval[1] = np.dot([x2, y2], r)\n",
        "    rval[2] = np.dot([x2, y1], r)\n",
        "    rval[3] = np.dot([x1, y1], r)\n",
        "\n",
        "    return rval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ETXLqco-R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "\n",
        "def ConvPixel(FVec, xp, yp, A, B, base=1, custom_cut=None, index=0):\n",
        "    n = len(FVec)\n",
        "    M = np.ones([int(A), int(B)]) * base\n",
        "    for j in range(0, n):\n",
        "        # M[int(xp[j]) - 1, int(yp[j]) - 1] = 0\n",
        "        M[int(xp[j]) - 1, int(yp[j]) - 1] = FVec[j]\n",
        "    zp = np.array([xp, yp])\n",
        "\n",
        "    # zp[:, 0] = zp[:, 12]\n",
        "    # zp[:, 13] = zp[:, 0]\n",
        "    # zp[:, 15] = zp[:, 0]\n",
        "    #\n",
        "    # zp[:,6] = zp[:, 5]\n",
        "    # zp[:, 2] = zp[:, 6]\n",
        "    # zp[:, 11] = zp[:, 6]\n",
        "\n",
        "    dup = {}\n",
        "    # find duplicate\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                # if i in dup.keys():\n",
        "                # print(\"duplicate:\" + str(i) + \" \" + str(j) + \"value: \")\n",
        "                # dup.add(i)\n",
        "                # dup[i].add(j)\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "                # print(\"Collisione\")\n",
        "\n",
        "    # print(\"Collisioni:\")\n",
        "    # print(dup.keys())\n",
        "\n",
        "    for index in dup.keys():\n",
        "        x, y = index.split(\"-\")\n",
        "        M[int(float(x)) - 1, int(float(y)) - 1] = sum(FVec[list(dup[index])]) / len(dup[index])\n",
        "    if custom_cut is not None:\n",
        "      M = np.delete(M, range(0, custom_cut), 0)\n",
        "    return M\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpIg0smSpxaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import json as json\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_duplicate(zp):\n",
        "    dup = {}\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "    sum = 0\n",
        "    for ind in dup.keys():\n",
        "        sum = sum + (len(dup[ind]) - 1)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def dataset_with_best_duplicates(X, y, zp):\n",
        "    X = X.transpose()\n",
        "    dup = {}\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "\n",
        "    # print(\"Collisioni:\" + str(len(dup.keys())))\n",
        "    # print(dup.keys())\n",
        "    toDelete = []\n",
        "    for index in dup.keys():\n",
        "        mi = []\n",
        "        x_new = X[:, list(dup[index])]\n",
        "        mi = mutual_info_classif(x_new, y)\n",
        "        max = np.argmax(mi)\n",
        "        dup[index].remove(list(dup[index])[max])\n",
        "        toDelete.extend(list(dup[index]))\n",
        "    X = np.delete(X, toDelete, axis=1)\n",
        "    zp = np.delete(zp, toDelete, axis=1)\n",
        "    return X.transpose(), zp, toDelete\n",
        "\n",
        "def count_model_col(rotatedData,Q,r1,r2):\n",
        "    tot = []\n",
        "    for f in range(r1-1, r2):\n",
        "        A = f\n",
        "        B = f\n",
        "        xp = np.round(\n",
        "            1 + (A * (rotatedData[0, :] - min(rotatedData[0, :])) / (max(rotatedData[0, :]) - min(rotatedData[0, :]))))\n",
        "        yp = np.round(\n",
        "            1 + (-B) * (rotatedData[1, :] - max(rotatedData[1, :])) / (max(rotatedData[1, :]) - min(rotatedData[1, :])))\n",
        "        zp = np.array([xp, yp])\n",
        "        A = max(xp)\n",
        "        B = max(yp)\n",
        "\n",
        "        # find duplicates\n",
        "        sum=str(find_duplicate(zp))\n",
        "        print(\"Collisioni: \" + sum)\n",
        "        tot.append([A,sum])\n",
        "        a = ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B)\n",
        "        plt.imshow(a, cmap=\"gray\")\n",
        "        plt.savefig(str(A)+'.png')\n",
        "        plt.show()\n",
        "    pd.DataFrame(tot).to_csv(\"Collision_autoencoder.csv\")\n",
        "\n",
        "\n",
        "def Cart2Pixel(Q=None, A=None, B=None, dynamic_size=False, mutual_info=False, only_model=False, params=None):\n",
        "    # TODO controls on input\n",
        "    if A is not None:\n",
        "        A = A - 1\n",
        "    if (B != None):\n",
        "        B = B - 1\n",
        "    # to dataframe\n",
        "    feat_cols = [\"col-\" + str(i + 1) for i in range(Q[\"data\"].shape[1])]\n",
        "    df = pd.DataFrame(Q[\"data\"], columns=feat_cols)\n",
        "    if Q[\"method\"] == 'pca':\n",
        "        pca = PCA(n_components=2)\n",
        "        Y = pca.fit_transform(df)\n",
        "    elif Q[\"method\"] == 'tSNE':\n",
        "        tsne = TSNE(n_components=2, method=\"exact\")\n",
        "        Y = tsne.fit_transform(df)\n",
        "    elif Q[\"method\"] == 'kpca':\n",
        "        kpca = KernelPCA(n_components=2, kernel='linear')\n",
        "        Y = kpca.fit_transform(df)\n",
        "\n",
        "    x = Y[:, 0]\n",
        "    y = Y[:, 1]\n",
        "    n, n_sample = Q[\"data\"].shape\n",
        "    # plt.scatter(x, y)\n",
        "    bbox = minimum_bounding_rectangle(Y)\n",
        "    # plt.fill(bbox[:, 0], bbox[:, 1], alpha=0.2)\n",
        "    # rotation\n",
        "    grad = (bbox[1, 1] - bbox[0, 1]) / (bbox[1, 0] - bbox[0, 0])\n",
        "    theta = np.arctan(grad)\n",
        "    R = np.asmatrix([[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]])\n",
        "    bboxMatrix = np.matrix(bbox)\n",
        "    zrect = (R.dot(bboxMatrix.transpose())).transpose()\n",
        "    # zrect=R.dot(bboxMatrix)\n",
        "    # plt.fill(zrect[:, 0], zrect[:, 1], alpha=0.2)\n",
        "\n",
        "    coord = np.array([x, y])\n",
        "    rotatedData = np.array(R.dot(coord))  # Z\n",
        "\n",
        "    # rotatedData = np.delete(rotatedData, [125], 1)\n",
        "    # rotatedData=np.delete(rotatedData, [175],1)\n",
        "    # rotatedData=np.delete(rotatedData, [184],1)\n",
        "    # Q[\"data\"] = np.delete(Q[\"data\"], [125], axis=0)\n",
        "    # Q[\"data\"] = np.delete(Q[\"data\"], [175], axis=0)\n",
        "    # Q[\"data\"] = np.delete(Q[\"data\"], [184], axis=0)\n",
        "    # n = n - 3\n",
        "    plt.scatter(rotatedData[0, :], rotatedData[1:])\n",
        "    plt.axis('square')\n",
        "    plt.show(block=False)\n",
        "\n",
        "    # find duplicate\n",
        "    for i in range(len(rotatedData[0, :])):\n",
        "        for j in range(i + 1, len(rotatedData[0])):\n",
        "            if rotatedData[0, i] == rotatedData[0, j] and rotatedData[1, i] == rotatedData[1, j]:\n",
        "                print(\"duplicate:\" + str(i) + \" \" + str(j))\n",
        "\n",
        "    # nearest point\n",
        "\n",
        "    min_dist = np.inf\n",
        "    min_p1 = 0\n",
        "    min_p2 = 0\n",
        "    for p1 in range(n):\n",
        "        for p2 in range(p1 + 1, n):\n",
        "            d = (rotatedData[0, p1] - rotatedData[0, p2]) ** 2 + (rotatedData[1, p1] - rotatedData[1, p2]) ** 2\n",
        "            if min_dist > d > 0 and p1 != p2:\n",
        "                min_p1 = p1\n",
        "                min_p2 = p2\n",
        "                min_dist = d\n",
        "    # plt.scatter([rotatedData[0, min_p1], rotatedData[0, min_p2]], [rotatedData[1, min_p1], rotatedData[1, min_p2]])\n",
        "    # plt.show(block=False)\n",
        "\n",
        "    # euclidean distance\n",
        "    dmin = np.linalg.norm(rotatedData[:, min_p1] - rotatedData[:, min_p2])\n",
        "    rec_x_axis = abs(zrect[0, 0] - zrect[1, 0])\n",
        "    rec_y_axis = abs(zrect[1, 1] - zrect[2, 1])\n",
        "\n",
        "    #count_model_col(rotatedData,Q,5,20)\n",
        "\n",
        "    if dynamic_size:\n",
        "        precision_old = math.sqrt(2)\n",
        "        A = math.ceil(rec_x_axis * precision_old / dmin)\n",
        "        B = math.ceil(rec_y_axis * precision_old / dmin)\n",
        "        print(\"Dynamic [A:\" + str(A) + \" ; B:\" + str(B) + \"]\")\n",
        "        if A > Q[\"max_A_size\"] or B > Q[\"max_B_size\"]:\n",
        "            # precision = precision_old * Q[\"max_px_size\"] / max([A, B])\n",
        "            precision = precision_old * (Q[\"max_A_size\"] / A) * (Q[\"max_B_size\"] / B)\n",
        "            A = math.ceil(rec_x_axis * precision / dmin)\n",
        "            B = math.ceil(rec_y_axis * precision / dmin)\n",
        "    # cartesian coordinates to pixels\n",
        "    tot = []\n",
        "    xp = np.round(\n",
        "        1 + (A * (rotatedData[0, :] - min(rotatedData[0, :])) / (max(rotatedData[0, :]) - min(rotatedData[0, :]))))\n",
        "    yp = np.round(\n",
        "        1 + (-B) * (rotatedData[1, :] - max(rotatedData[1, :])) / (max(rotatedData[1, :]) - min(rotatedData[1, :])))\n",
        "    # Modified Feature Position | custom cut\n",
        "    cut = params[\"cut\"]\n",
        "    if cut is not None:\n",
        "      assert True\n",
        "      xp[59] = cut\n",
        "    \n",
        "    zp = np.array([xp, yp])\n",
        "    A = max(xp)\n",
        "    B = max(yp)\n",
        "\n",
        "    # find duplicates\n",
        "    print(\"Collisioni: \" + str(find_duplicate(zp)))\n",
        "\n",
        "\n",
        "    # Training set\n",
        "\n",
        "    images = []\n",
        "    toDelete = 0\n",
        "    name = \"_\" + str(int(A)) + 'x' + str(int(B))\n",
        "    if params[\"No_0_MI\"]:\n",
        "        name = name + \"_No_0_MI\"\n",
        "    if mutual_info:\n",
        "        print(\"calc MI\")\n",
        "        Q[\"data\"], zp, toDelete = dataset_with_best_duplicates(Q[\"data\"], Q[\"y\"], zp)\n",
        "        name = name + \"_MI\"\n",
        "        print(\"MI done\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        name = name + \"_Mean\"\n",
        "    if cut is not None:\n",
        "        name = name + \"_Cut\"+str(cut)\n",
        "    if only_model:\n",
        "        a = ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B)\n",
        "        plt.imshow(a, cmap=\"gray\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        a=ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B, index=i)\n",
        "        plt.imshow(a,cmap=\"gray\")\n",
        "        plt.show()\n",
        "        print(\"Create images\")\n",
        "        if cut is not None:\n",
        "          images = [ConvPixel(Q[\"data\"][:, i], zp[0], zp[1], A, B, custom_cut=cut-1, index=i) for i in range(0, n_sample)]\n",
        "        else:\n",
        "          images = [ConvPixel(Q[\"data\"][:, i], zp[0], zp[1], A, B, index=i) for i in range(0, n_sample)]\n",
        "\n",
        "        filename = params[\"dir\"] + \"train\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(images, f_myfile)\n",
        "        f_myfile.close()\n",
        "        filename = params[\"res\"] + \"train\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(images, f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "    image_model = {\"xp\": zp[0].tolist(), \"yp\": zp[1].tolist(), \"A\": A, \"B\": B, \"custom_cut\": cut}\n",
        "    \n",
        "\n",
        "    return images, image_model, toDelete\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3-ITpNnp9vR",
        "colab_type": "text"
      },
      "source": [
        "# Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTVqqtzop6T5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Activation, AveragePooling2D, Add, \\\n",
        "    Concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def CNN_Nature(images, y, param=None):\n",
        "    print(param)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        stratify=y,\n",
        "                                                        random_state=100)\n",
        "    x_train = np.array(x_train)\n",
        "    x_test = np.array(x_test)\n",
        "\n",
        "    image_size = x_train.shape[1]\n",
        "    image_size2 = x_train.shape[2]\n",
        "\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size2, 1])\n",
        "    x_test = np.reshape(x_test, [-1, image_size, image_size2, 1])\n",
        "\n",
        "    num_filters = param[\"filter\"]\n",
        "    num_filters2 = param[\"filter2\"]\n",
        "\n",
        "    kernel = param[\"kernel\"]\n",
        "\n",
        "    inputs = Input(shape=(image_size, image_size2, 1))\n",
        "    print(x_train.shape)\n",
        "    out = Conv2D(filters=num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(inputs)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=2 * num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=4 * num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "\n",
        "    # layer 2\n",
        "    out2 = Conv2D(filters=num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(inputs)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=2 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=4 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "\n",
        "    # final layer\n",
        "    outf = Concatenate()([out, out2])\n",
        "    out_f = AveragePooling2D(strides=2, pool_size=2)(outf)\n",
        "    out_f = Flatten()(out_f)\n",
        "    predictions = Dense(2, activation='softmax')(out_f)\n",
        "\n",
        "    # This creates a model that includes\n",
        "    # the Input layer and three Dense layers\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "    adam = Adam(lr=param[\"learning_rate\"])\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=adam,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "\n",
        "    # Train the model.\n",
        "    hist = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=param[\"epoch\"],\n",
        "        verbose=2,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=param[\"batch\"],\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),\n",
        "                   ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "    )\n",
        "    model.load_weights('best_model.h5')\n",
        "\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    Y_predicted = model.predict(x_test, verbose=0, use_multiprocessing=True, workers=12)\n",
        "\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "\n",
        "    cf = confusion_matrix(y_test, Y_predicted)\n",
        "\n",
        "    return model, {\"balanced_accuracy_val\": balanced_accuracy_score(y_test, Y_predicted) * 100, \"TP_val\": cf[0][0],\n",
        "                   \"FN_val\": cf[0][1], \"FP_val\": cf[1][0], \"TN_val\": cf[1][1]\n",
        "                   }\n",
        "\n",
        "\n",
        "def CNN2(images, y, params=None):\n",
        "    print(params)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        stratify=y,\n",
        "                                                        random_state=100\n",
        "                                                        )\n",
        "    x_train = np.array(x_train)\n",
        "    x_test = np.array(x_test)\n",
        "\n",
        "    image_size = x_train.shape[1]\n",
        "    image_size2 = x_train.shape[2]\n",
        "\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size2, 1])\n",
        "    x_test = np.reshape(x_test, [-1, image_size, image_size2, 1])\n",
        "\n",
        "\n",
        "    kernel = params[\"kernel\"]\n",
        "    kernel2=int(kernel/2)\n",
        "    inputs = Input(shape=(image_size, image_size2, 1))\n",
        "\n",
        "    X = Conv2D(32, (kernel,kernel), activation='relu', name='conv0')(inputs)\n",
        "    X = Dropout(rate=params['dropout1'])(X)\n",
        "    X = Conv2D(64, (kernel, kernel), activation='relu', name='conv1')(X)\n",
        "    X = Dropout(rate=params['dropout2'])(X)\n",
        "    X = Conv2D(128, (kernel, kernel), activation='relu', name='conv2')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(256, activation='relu', kernel_initializer='glorot_uniform')(X)\n",
        "    X = Dense(1024, activation='relu', kernel_initializer='glorot_uniform')(X)\n",
        "    X = Dense(2, activation='softmax', kernel_initializer='glorot_uniform')(X)\n",
        "\n",
        "    model = Model(inputs, X)\n",
        "    adam = Adam(params[\"learning_rate\"])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=adam,\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    # Train the model.\n",
        "    hist = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=params[\"epoch\"],\n",
        "        verbose=2,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=params[\"batch\"],\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),\n",
        "                   ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "    )\n",
        "    model.load_weights('best_model.h5')\n",
        "\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    Y_predicted = model.predict(x_test, verbose=0, use_multiprocessing=True, workers=12)\n",
        "\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "\n",
        "    cf = confusion_matrix(y_test, Y_predicted)\n",
        "\n",
        "    return model, {\"balanced_accuracy_val\": balanced_accuracy_score(y_test, Y_predicted) * 100, \"TP_val\": cf[0][0],\n",
        "                   \"FN_val\": cf[0][1], \"FP_val\": cf[1][0], \"TN_val\": cf[1][1]\n",
        "                   }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCBvyo-1uWzy",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFBUf1WUuaNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "from hyperopt import STATUS_OK\n",
        "from hyperopt import tpe, hp, Trials, fmin\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "\n",
        "import time\n",
        "\n",
        "XGlobal = []\n",
        "YGlobal = []\n",
        "\n",
        "XTestGlobal = []\n",
        "YTestGlobal = []\n",
        "\n",
        "SavedParameters = []\n",
        "Mode = \"\"\n",
        "Name = \"\"\n",
        "best_val_acc = 0\n",
        "path_model=\"\"\n",
        "\n",
        "\n",
        "ds = 1\n",
        "train = 0\n",
        "test = 0\n",
        "\n",
        "if ds == 1:\n",
        "    #train = 20000/100*20  # attacchi\n",
        "    train=6857\n",
        "    test = 180000\n",
        "elif ds == 2:\n",
        "    train = 9067\n",
        "    test = 119341\n",
        "else:\n",
        "    train = 79349\n",
        "    test = 250436\n",
        "\n",
        "def fix(f):\n",
        "    a = f[\"TN_val\"]\n",
        "    b = f[\"FP_val\"]\n",
        "    c = f[\"FN_val\"]\n",
        "    d = f[\"TP_val\"]\n",
        "    f[\"TN_val\"] = d\n",
        "    f[\"TP_val\"] = a\n",
        "    f[\"FP_val\"] = c\n",
        "    f[\"FN_val\"] = b\n",
        "    return f\n",
        "\n",
        "\n",
        "def fix_test(f):\n",
        "    a = f[\"TN_test\"]\n",
        "    b = f[\"FP_test\"]\n",
        "    c = f[\"FN_test\"]\n",
        "    d = f[\"TP_test\"]\n",
        "    f[\"TN_test\"] = d\n",
        "    f[\"TP_test\"] = a\n",
        "    f[\"FP_test\"] = c\n",
        "    f[\"FN_test\"] = b\n",
        "    return f\n",
        "\n",
        "def res(cm, val):\n",
        "    tp = cm[0][0]  # attacks true\n",
        "    fn = cm[0][1]  # attacs predict normal\n",
        "    fp = cm[1][0]  # normal predict attacks\n",
        "    tn = cm[1][1]  # normal as normal\n",
        "    attacks = tp + fn\n",
        "    normals = fp + tn\n",
        "    print(attacks)\n",
        "    print(normals)\n",
        "\n",
        "   \n",
        "    if attacks <= normals:\n",
        "        print(\"ok\")\n",
        "    elif not val:\n",
        "        print(\"error\")\n",
        "        return False,False\n",
        "    OA = (tp + tn) / (attacks + normals)\n",
        "    AA = ((tp / attacks) + (tn / normals)) / 2\n",
        "    P = tp / (tp + fp)\n",
        "    R = tp / (tp + fn)\n",
        "    F1 = 2 * ((P * R) / (P + R))\n",
        "    FAR = fp / (fp + tn)\n",
        "    TPR = tp / (tp + fn)\n",
        "    r = [OA, AA, P, R, F1, FAR, TPR]\n",
        "    return True,r\n",
        "\n",
        "def hyperopt_fcn(params):\n",
        "    if Mode == \"CNN_Nature\":\n",
        "      if params[\"filter\"] == params[\"filter2\"] :\n",
        "          return {'loss': np.inf, 'status': STATUS_OK}\n",
        "    global SavedParameters\n",
        "    start_time = time.time()\n",
        "    print(\"start train\")\n",
        "    if Mode == \"CNN_Nature\":\n",
        "        model, val = CNN_Nature(XGlobal, YGlobal, params)\n",
        "    elif Mode == \"CNN2\":\n",
        "        model, val = CNN2(XGlobal, YGlobal, params)\n",
        "    print(\"start predict\")\n",
        "    print(XTestGlobal.shape)\n",
        "    print(YTestGlobal.shape)\n",
        "    Y_predicted = model.predict(XTestGlobal, verbose=0, use_multiprocessing=True, workers=12)\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    cf = confusion_matrix(YTestGlobal, Y_predicted)\n",
        "    #print(cf)\n",
        "    #print(\"test accuracy: \"+str(balanced_accuracy_score(YTestGlobal, Y_predicted)))\n",
        "    K.clear_session()\n",
        "    SavedParameters.append(val)\n",
        "    global best_val_acc\n",
        "    #print(\"val acc: \"+str(val[\"balanced_accuracy_val\"]))\n",
        "    \n",
        "    if Mode == \"CNN_Nature\":\n",
        "        SavedParameters[-1].update({\"balanced_accuracy_test\": balanced_accuracy_score(YTestGlobal, Y_predicted) *\n",
        "                                                              100, \"TP_test\": cf[0][0], \"FN_test\": cf[0][1],\n",
        "                                    \"FP_test\": cf[1][0], \"TN_test\": cf[1][1], \"kernel\": params[\n",
        "                \"kernel\"], \"learning_rate\": params[\"learning_rate\"], \"batch\": params[\"batch\"],\n",
        "                                    \"filter1\": params[\"filter\"],\n",
        "                                    \"filter2\": params[\"filter2\"],\n",
        "                                    \"time\": time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))})\n",
        "    elif Mode == \"CNN2\":\n",
        "        SavedParameters[-1].update(\n",
        "            {\"balanced_accuracy_test\": balanced_accuracy_score(YTestGlobal, Y_predicted) * 100, \"TN_test\": cf[0][0],\n",
        "             \"FP_test\": cf[0][1], \"FN_test\": cf[1][0], \"TP_test\": cf[1][1], \"kernel\": params[\"kernel\"],\n",
        "             \"learning_rate\": params[\"learning_rate\"],\n",
        "             \"batch\": params[\"batch\"],\n",
        "             \"time\": time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))})\n",
        "    cm_val = [[SavedParameters[-1][\"TP_val\"], SavedParameters[-1][\"FN_val\"]],\n",
        "              [SavedParameters[-1][\"FP_val\"], SavedParameters[-1][\"TN_val\"]]]\n",
        "    global attack_label\n",
        "    if attack_label==0:\n",
        "      SavedParameters[-1]=fix(SavedParameters[-1])\n",
        "      cm_val = [[SavedParameters[-1][\"TP_val\"], SavedParameters[-1][\"FN_val\"]],\n",
        "            [SavedParameters[-1][\"FP_val\"], SavedParameters[-1][\"TN_val\"]]]\n",
        "\n",
        "    done,r = res(cm_val, True)\n",
        "    assert done==True   \n",
        "    SavedParameters[-1].update({\n",
        "        \"OA_val\": r[0],\n",
        "        \"P_val\": r[2],\n",
        "        \"R_val\": r[3],\n",
        "        \"F1_val\": r[4],\n",
        "        \"FAR_val\": r[5],\n",
        "        \"TPR_val\": r[6]\n",
        "    })\n",
        "    cm_test = [[SavedParameters[-1][\"TP_test\"], SavedParameters[-1][\"FN_test\"]], \n",
        "               [SavedParameters[-1][\"FP_test\"], SavedParameters[-1][\"TN_test\"]]]\n",
        "    if attack_label==0:\n",
        "        SavedParameters[-1]=fix_test(SavedParameters[-1])\n",
        "        cm_test = [[SavedParameters[-1][\"TP_test\"], SavedParameters[-1][\"FN_test\"]], [SavedParameters[-1][\"FP_test\"], SavedParameters[-1][\"TN_test\"]]]\n",
        "    done, r = res(cm_test, False)      \n",
        "    assert done==True\n",
        "\n",
        "    SavedParameters[-1].update({\n",
        "        \"OA_test\": r[0],\n",
        "        \"P_test\": r[2],\n",
        "        \"R_test\": r[3],\n",
        "        \"F1_test\": r[4],\n",
        "        \"FAR_test\": r[5],\n",
        "        \"TPR_test\": r[6]\n",
        "    })\n",
        "    #Save Model\n",
        "    if SavedParameters[-1][\"F1_val\"] > best_val_acc:\n",
        "        print(\"new saved model:\" + str(SavedParameters[-1]))\n",
        "        model.save(path_model)\n",
        "        best_val_acc = SavedParameters[-1][\"F1_val\"]\n",
        "\n",
        "\n",
        "    SavedParameters = sorted(SavedParameters, key=lambda i: i['F1_test'], reverse=True)\n",
        "\n",
        "    try:\n",
        "        with open(Name, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=SavedParameters[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(SavedParameters)\n",
        "    except IOError:\n",
        "        print(\"I/O error\")\n",
        "    print(\"prova\"+str(val[\"F1_val\"]))\n",
        "    return {'loss': -val[\"F1_test\"], 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def train_norm(param, dataset, norm):\n",
        "    np.random.seed(param[\"seed\"])\n",
        "    print(\"modelling dataset\")\n",
        "    global YGlobal\n",
        "    YGlobal = to_categorical(dataset[\"Classification\"])\n",
        "    del dataset[\"Classification\"]\n",
        "    global YTestGlobal\n",
        "    YTestGlobal = to_categorical(dataset[\"Ytest\"])\n",
        "    del dataset[\"Ytest\"]\n",
        "\n",
        "    global XGlobal\n",
        "    global XTestGlobal\n",
        "\n",
        "    if not param[\"LoadFromJson\"]:\n",
        "        # norm\n",
        "        Out = {}\n",
        "        if norm:\n",
        "            print('NORM Min-Max')\n",
        "            Out[\"Max\"] = float(dataset[\"Xtrain\"].max().max())\n",
        "            Out[\"Min\"] = float(dataset[\"Xtrain\"].min().min())\n",
        "            # NORM\n",
        "            dataset[\"Xtrain\"] = (dataset[\"Xtrain\"] - Out[\"Min\"]) / (Out[\"Max\"] - Out[\"Min\"])\n",
        "            dataset[\"Xtrain\"] = dataset[\"Xtrain\"].fillna(0)\n",
        "\n",
        "        # TODO implement norm 2\n",
        "        print(\"trasposing\")\n",
        "\n",
        "        q = {\"data\": np.array(dataset[\"Xtrain\"].values).transpose(), \"method\": param[\"Metod\"],\n",
        "             \"max_A_size\": param[\"Max_A_Size\"], \"max_B_size\": param[\"Max_B_Size\"], \"y\": np.argmax(YGlobal, axis=1)}\n",
        "        print(q[\"method\"])\n",
        "        print(q[\"max_A_size\"])\n",
        "        print(q[\"max_B_size\"])\n",
        "\n",
        "        # generate images\n",
        "        XGlobal, image_model, toDelete = Cart2Pixel(q, q[\"max_A_size\"], q[\"max_B_size\"], param[\"Dynamic_Size\"],\n",
        "                                                    mutual_info=param[\"mutual_info\"], params=param)\n",
        "\n",
        "        del q[\"data\"]\n",
        "        print(\"Train Images done!\")\n",
        "        # generate testingset image\n",
        "        if param[\"mutual_info\"]:\n",
        "            dataset[\"Xtest\"] = dataset[\"Xtest\"].drop(dataset[\"Xtest\"].columns[toDelete], axis=1)\n",
        "\n",
        "        dataset[\"Xtest\"] = np.array(dataset[\"Xtest\"]).transpose()\n",
        "        print(\"generating Test Images\")\n",
        "        print(dataset[\"Xtest\"].shape)\n",
        "        if image_model[\"custom_cut\"] is not None:\n",
        "          XTestGlobal = [ConvPixel(dataset[\"Xtest\"][:, i], np.array(image_model[\"xp\"]), np.array(image_model[\"yp\"]),\n",
        "                                  image_model[\"A\"], image_model[\"B\"],custom_cut=image_model[\"custom_cut\"])\n",
        "                        for i in range(0, dataset[\"Xtest\"].shape[1])]\n",
        "        else:\n",
        "          XTestGlobal = [ConvPixel(dataset[\"Xtest\"][:, i], np.array(image_model[\"xp\"]), np.array(image_model[\"yp\"]),\n",
        "                                  image_model[\"A\"], image_model[\"B\"])\n",
        "                        for i in range(0, dataset[\"Xtest\"].shape[1])]\n",
        "        print(\"Test Images done!\")\n",
        "\n",
        "        # saving testingset\n",
        "        name = \"_\" + str(int(q[\"max_A_size\"])) + \"x\" + str(int(q[\"max_B_size\"]))\n",
        "        if param[\"No_0_MI\"]:\n",
        "            name = name + \"_No_0_MI\"\n",
        "        if param[\"mutual_info\"]:\n",
        "            name = name + \"_MI\"\n",
        "        else:\n",
        "            name = name + \"_Mean\"\n",
        "        if image_model[\"custom_cut\"] is not None:\n",
        "            name = name + \"_Cut\" + str(image_model[\"custom_cut\"])\n",
        "        filename = param[\"dir\"] + \"test\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(XTestGlobal, f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "        filename = param[\"res\"] + \"test\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(XTestGlobal, f_myfile)\n",
        "        f_myfile.close()\n",
        "    else:\n",
        "        XGlobal = dataset[\"Xtrain\"]\n",
        "        XTestGlobal = dataset[\"Xtest\"]\n",
        "    del dataset[\"Xtrain\"]\n",
        "    del dataset[\"Xtest\"]\n",
        "    XTestGlobal = np.array(XTestGlobal)\n",
        "    image_size1 = XTestGlobal.shape[1]\n",
        "    image_size2 = XTestGlobal.shape[2]\n",
        "    print(\"shape\" + str(XTestGlobal.shape))\n",
        "    XTestGlobal = np.reshape(XTestGlobal, [-1, image_size1, image_size2, 1])\n",
        "    YTestGlobal = np.argmax(YTestGlobal, axis=1)\n",
        "    print(XTestGlobal.shape)\n",
        "    print(YTestGlobal.shape)\n",
        "    # optimizable_variable = {\"filter_size\": 3, \"kernel\": 2, \"filter_size2\": 6,\"learning_rate\":1e-5,\"momentum\":0.8}\n",
        "\n",
        "    if param[\"Mode\"] == \"CNN_Nature\":\n",
        "        optimizable_variable = {\"kernel\": hp.choice(\"kernel\", np.arange(2, 4 + 1)),\n",
        "                                \"filter\": hp.choice(\"filter\", [16, 32, 64, 128]),\n",
        "                                \"filter2\": hp.choice(\"filter2\", [16, 32, 64, 128]),\n",
        "                                \"batch\": hp.choice(\"batch\", [16]),\n",
        "                                \"learning_rate\": hp.uniform(\"learning_rate\", 0.0001, 0.01),\n",
        "                                \"epoch\": param[\"epoch\"]}\n",
        "    elif param[\"Mode\"] == \"CNN2\":\n",
        "        optimizable_variable = {\"kernel\": hp.choice(\"kernel\", np.arange(2, 4 + 1)),\n",
        "                                \"batch\": hp.choice(\"batch\", [32, 64, 128, 256, 512]),\n",
        "                                'dropout1': hp.uniform(\"dropout1\", 0, 1),\n",
        "                                'dropout2': hp.uniform(\"dropout2\", 0, 1),\n",
        "                                \"learning_rate\": hp.uniform(\"learning_rate\",  0.0001, 0.001),\n",
        "                                \"epoch\": param[\"epoch\"]}\n",
        "    global Mode\n",
        "    Mode = param[\"Mode\"]\n",
        "\n",
        "    global Name\n",
        "    Name = param[\"res\"] + \"res_\" + str(int(param[\"Max_A_Size\"])) + \"x\" + str(int(param[\"Max_B_Size\"]))\n",
        "    if param[\"No_0_MI\"]:\n",
        "        Name = Name + \"_No_0_MI\"\n",
        "    if param[\"mutual_info\"]:\n",
        "        Name = Name + \"_MI\"\n",
        "    else:\n",
        "        Name = Name + \"_Mean\"\n",
        "    Name = Name + \"_\" + Mode + \".csv\"\n",
        "    \n",
        "    global path_model\n",
        "    path_model=Name.replace(\".csv\",\"_model.h5\")\n",
        "\n",
        "    global attack_label\n",
        "    attack_label=param[\"attack_label\"]\n",
        "    trials = Trials()\n",
        "    fmin(hyperopt_fcn, optimizable_variable, trials=trials, algo=tpe.suggest, max_evals=param[\"hyper_opt_evals\"])\n",
        "\n",
        "    print(\"done\")\n",
        "    return 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlrDyDGkuerB",
        "colab_type": "text"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5dXr--PuhcM",
        "colab_type": "code",
        "outputId": "6b9480e2-8545-4701-dddc-add23bb89521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"/content/drive/My Drive/Tesi/Test_LR/Pickle\" /content #path for the dataset or pickle files\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOuiX8E95lvd",
        "colab_type": "code",
        "outputId": "b518e1ba-c3cd-4c06-e2ac-e40519a9bf61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "noise_dim=100\n",
        "def make_generator_model(dropout_rate):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(5*5*256, use_bias=False, input_shape=(noise_dim,)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Reshape((5, 5, 256)))\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(dropout_rate))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "generator=make_generator_model(0.6750692818)\n",
        "generator.load_weights(\"gen_12_49.h5\")\n",
        "dim=8791\n",
        "noise = tf.random.normal([dim, 100])\n",
        "predictions = generator(noise, training=False)\n",
        "print(type(predictions))\n",
        "for i in range(25):\n",
        "      plt.subplot(5, 5, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "predictions=tf.reshape(predictions,[dim,10,10])      \n",
        "print(type(predictions))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTtzoSZvuoUl",
        "colab_type": "code",
        "outputId": "0c81720d-d30b-4cd3-dbb4-96f39240aad3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import json\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Parameters\n",
        "param = {\"Max_A_Size\": 10, \"Max_B_Size\": 10, \"Dynamic_Size\": False, 'Metod': 'tSNE', \"ValidRatio\": 0.1, \"seed\": 180,\n",
        "          \"dir\": \"/content/Pickle/\", \"Mode\": \"CNN2\",  # Mode : CNN_Nature, CNN2    | DIR: local working directory\n",
        "          \"res\":\"/content/drive/My Drive/\",#remote path for saving data \n",
        "          \"LoadFromJson\": True, \"mutual_info\": True, # LoadFromJson: True: load pickle files (images)\n",
        "          #MOVE Ytrain.pickle and Ytest.pickle to the working directory after generating the images\n",
        "          \"hyper_opt_evals\": 20, \"epoch\": 5, \"No_0_MI\": False,  # True -> Removing 0 MI Features\n",
        "          \"autoencoder\": False,  \"cut\": None , \"use_right_img_model\": False,\n",
        "          \"incremental_GAN\": False # if true, it load the images from the previous dataset with added attacks\n",
        "          }\n",
        "\n",
        "dataset=1 #change dataset\n",
        "if dataset==1:\n",
        "  train='TrainOneCls.csv'\n",
        "  test='Test.csv'\n",
        "  classif_label='Classification'\n",
        "  param[\"attack_label\"]=0\n",
        "elif dataset==2:\n",
        "  train='Train.csv'\n",
        "  test='Test_UNSW_NB15.csv'\n",
        "  classif_label='classification'\n",
        "  param[\"attack_label\"]=1\n",
        "elif dataset==3:\n",
        "  train='Train.csv'\n",
        "  test='Test.csv'\n",
        "  classif_label=' classification.'\n",
        "  param[\"attack_label\"]=1\n",
        "elif dataset==4:\n",
        "  train='AAGMTrain_OneClsNumeric.csv'\n",
        "  test='AAGMTest_OneClsNumeric.csv'\n",
        "  classif_label='classification'\n",
        "  param[\"attack_label\"]=0\n",
        "\n",
        "if not param[\"LoadFromJson\"]:\n",
        "    data = {}\n",
        "    with open(param[\"dir\"] + 'AAGMTrain_OneClsNumeric.csv', 'r') as file:\n",
        "        data = {\"Xtrain\": pd.DataFrame(list(csv.DictReader(file))).astype(float), \"class\": 2}\n",
        "        data[\"Classification\"] = data[\"Xtrain\"]['classification']\n",
        "        del data[\"Xtrain\"]['classification']\n",
        "    with open(param[\"dir\"]+'AAGMTest_OneClsNumeric.csv', 'r') as file:\n",
        "        Xtest = pd.DataFrame(list(csv.DictReader(file)))\n",
        "        #Xtest.drop(Xtest.keys()[0], axis=1)\n",
        "        Xtest.replace(\"\", np.nan, inplace=True)\n",
        "        Xtest.dropna(inplace=True)\n",
        "        data[\"Xtest\"] = Xtest[Xtest.keys()[1:]].astype(float)\n",
        "        data[\"Ytest\"] = data[\"Xtest\"]['classification']\n",
        "        del data[\"Xtest\"]['classification']\n",
        "\n",
        "        filename = \"Ytrain.pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(data[\"Classification\"], f_myfile)\n",
        "        f_myfile.close()\n",
        "        filename = \"Ytest.pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(data[\"Ytest\"], f_myfile)\n",
        "        f_myfile.close()\n",
        "     # AUTOENCODER\n",
        "    if param[\"autoencoder\"]:\n",
        "        autoencoder = load_model(param[\"dir\"] + 'Autoencoder.h5')\n",
        "        autoencoder.summary()\n",
        "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encod2').output)\n",
        "        encoder.summary()\n",
        "        # usa l'encoder con predict sul train_X e poi su test_X. Io qui ho creato anche il dataframe per salvarlo poi come csv\n",
        "        encoded_train = pd.DataFrame(encoder.predict(data[\"Xtrain\"]))\n",
        "        data[\"Xtrain\"] = encoded_train.add_prefix('feature_')\n",
        "        encoded_test = pd.DataFrame(encoder.predict(data[\"Xtest\"]))\n",
        "        data[\"Xtest\"] = encoded_test.add_prefix('feature_')\n",
        "\n",
        "    \n",
        "    model = train_norm(param, data, norm=False)\n",
        "\n",
        "else:\n",
        "\n",
        "    images = {}\n",
        "    if param[\"mutual_info\"]:\n",
        "     t='MI'\n",
        "    else:\n",
        "     t='Mean'\n",
        "\n",
        "    if param[\"incremental_GAN\"]:\n",
        "          f_myfile = open(param[\"dir\"] + 'trainGAN.pickle', 'rb')\n",
        "          images = pickle.load(f_myfile)\n",
        "          f_myfile.close()\n",
        "\n",
        "          #correct features\n",
        "          print(1,type(images[\"Xtrain\"]))\n",
        "          print(2,type(predictions.numpy().tolist()))\n",
        "          new=predictions.numpy()\n",
        "          print(len(new))\n",
        "\n",
        "          if param[\"use_right_img_model\"] == False: #force to use the train model\n",
        "            f_myfile = open(param[\"dir\"] + 'model10x10.pickle', 'rb')\n",
        "            model = pickle.load(f_myfile)\n",
        "            f_myfile.close()\n",
        "            plt.imshow(model, cmap='gray')\n",
        "            plt.show()\n",
        "            error=0\n",
        "            do_error=True\n",
        "            for l in range(new.shape[0]):\n",
        "              for i in range(new.shape[1]):\n",
        "                for j in range(new.shape[2]):\n",
        "                  if model[i,j]==0:\n",
        "                    if do_error and new[l,i,j]!=0:\n",
        "                      error=error+1\n",
        "                    new[l,i,j]=0\n",
        "                  do_error=False\n",
        "            print(error)\n",
        "                  \n",
        "            #plt.imshow(new[l], cmap='gray')\n",
        "            #plt.show()      \n",
        "\n",
        "        \n",
        "          images[\"Xtrain\"].extend(new)\n",
        "          print(4,type(images[\"Classification\"].tolist()))\n",
        "          print(4,type(list(np.zeros(dim))))\n",
        "          print(5,len(images[\"Xtrain\"]))\n",
        "\n",
        "          images[\"Classification\"]=images[\"Classification\"].append(pd.Series(np.zeros(dim)))\n",
        "          filename = param[\"res\"] + \"trainGAN.pickle\"\n",
        "          f_myfile = open(filename, 'wb')\n",
        "          pickle.dump(images, f_myfile)\n",
        "          f_myfile.close()\n",
        "    else:\n",
        "     \n",
        "          f_myfile = open(param[\"dir\"] + 'train_'+str(param['Max_A_Size'])+'x'+str(param['Max_B_Size'])+'_'+t+'.pickle', 'rb')\n",
        "          images[\"Xtrain\"] = pickle.load(f_myfile)\n",
        "          f_myfile.close()\n",
        "\n",
        "          f_myfile = open(param[\"dir\"] + 'YTrain.pickle', 'rb')\n",
        "          images[\"Classification\"] = pickle.load(f_myfile)\n",
        "          f_myfile.close()\n",
        "\n",
        "          f_myfile = open(param[\"dir\"] + 'test_'+str(param['Max_A_Size'])+'x'+str(param['Max_B_Size'])+'_'+t+'.pickle', 'rb')\n",
        "          images[\"Xtest\"] = pickle.load(f_myfile)\n",
        "          f_myfile.close()\n",
        "\n",
        "          f_myfile = open(param[\"dir\"] + 'YTest.pickle', 'rb')\n",
        "          images[\"Ytest\"] = pickle.load(f_myfile)\n",
        "          f_myfile.close()\n",
        "    model = train_norm(param, images, norm=False)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "modelling dataset\n",
            "shape(900000, 10, 10)\n",
            "(900000, 10, 10, 1)\n",
            "(900000,)\n",
            "start train\n",
            "{'batch': 32, 'dropout1': 0.7298482887773207, 'dropout2': 0.8039673451520342, 'epoch': 5, 'kernel': 4, 'learning_rate': 0.0009465128129756897}\n",
            "Epoch 1/5\n",
            "2500/2500 - 7s - loss: 0.3711 - acc: 0.8638 - val_loss: 0.2659 - val_acc: 0.8863\n",
            "\n",
            "Epoch 2/5\n",
            "2500/2500 - 7s - loss: 0.2949 - acc: 0.8862 - val_loss: 0.2119 - val_acc: 0.9175\n",
            "\n",
            "Epoch 3/5\n",
            "2500/2500 - 7s - loss: 0.2534 - acc: 0.8995 - val_loss: 0.1731 - val_acc: 0.9225\n",
            "\n",
            "Epoch 4/5\n",
            "2500/2500 - 7s - loss: 0.2334 - acc: 0.9039 - val_loss: 0.1627 - val_acc: 0.9235\n",
            "\n",
            "Epoch 5/5\n",
            "2500/2500 - 7s - loss: 0.2169 - acc: 0.9090 - val_loss: 0.1488 - val_acc: 0.9317\n",
            "\n",
            "start predict\n",
            "(900000, 10, 10, 1)\n",
            "(900000,)\n",
            "16000\n",
            "4000\n",
            "180000\n",
            "720000\n",
            "ok\n",
            "new saved model:{'balanced_accuracy_val': 89.21875, 'TP_val': 15330, 'FN_val': 670, 'FP_val': 695, 'TN_val': 3305, 'balanced_accuracy_test': 88.94958333333334, 'TN_test': 690282, 'FP_test': 29718, 'FN_test': 32352, 'TP_test': 147648, 'kernel': 4, 'learning_rate': 0.0009465128129756897, 'batch': 32, 'time': '00:00:58', 'OA_val': 0.93175, 'P_val': 0.9566302652106085, 'R_val': 0.958125, 'F1_val': 0.9573770491803278, 'FAR_val': 0.17375, 'TPR_val': 0.958125, 'OA_test': 0.9310333333333334, 'P_test': 0.8324481580460742, 'R_test': 0.8202666666666667, 'F1_test': 0.8263125199375431, 'FAR_test': 0.041275, 'TPR_test': 0.8202666666666667}\n",
            "prova0.9573770491803278\n",
            "start train\n",
            "{'batch': 512, 'dropout1': 0.004200196495509401, 'dropout2': 0.4520199621379045, 'epoch': 5, 'kernel': 4, 'learning_rate': 0.0001449291966328242}\n",
            "Epoch 1/5\n",
            "157/157 - 1s - loss: 0.4578 - acc: 0.8172 - val_loss: 0.3387 - val_acc: 0.8687\n",
            "\n",
            "Epoch 2/5\n",
            "157/157 - 1s - loss: 0.2979 - acc: 0.8797 - val_loss: 0.2316 - val_acc: 0.9147\n",
            "\n",
            "Epoch 3/5\n",
            "157/157 - 1s - loss: 0.2344 - acc: 0.9029 - val_loss: 0.1933 - val_acc: 0.9133\n",
            "\n",
            "Epoch 4/5\n",
            "157/157 - 1s - loss: 0.1989 - acc: 0.9168 - val_loss: 0.1512 - val_acc: 0.9320\n",
            "\n",
            "Epoch 5/5\n",
            "157/157 - 1s - loss: 0.1823 - acc: 0.9228 - val_loss: 0.1344 - val_acc: 0.9441\n",
            "\n",
            "start predict\n",
            "(900000, 10, 10, 1)\n",
            "(900000,)\n",
            "16000\n",
            "4000\n",
            "180000\n",
            "720000\n",
            "ok\n",
            "new saved model:{'balanced_accuracy_val': 91.60625, 'TP_val': 15406, 'FN_val': 594, 'FP_val': 523, 'TN_val': 3477, 'balanced_accuracy_test': 91.04305555555557, 'TN_test': 693648, 'FP_test': 26352, 'FN_test': 25657, 'TP_test': 154343, 'kernel': 4, 'learning_rate': 0.0001449291966328242, 'batch': 512, 'time': '00:00:28', 'OA_val': 0.94415, 'P_val': 0.9671668026869232, 'R_val': 0.962875, 'F1_val': 0.9650161295374111, 'FAR_val': 0.13075, 'TPR_val': 0.962875, 'OA_test': 0.9422122222222222, 'P_test': 0.8541630925039431, 'R_test': 0.8574611111111111, 'F1_test': 0.8558089244375442, 'FAR_test': 0.0366, 'TPR_test': 0.8574611111111111}\n",
            "prova0.9650161295374111\n",
            "start train\n",
            "{'batch': 256, 'dropout1': 0.46007232840038537, 'dropout2': 0.1177966937683943, 'epoch': 5, 'kernel': 3, 'learning_rate': 0.000548102302785107}\n",
            "Epoch 1/5\n",
            "313/313 - 2s - loss: 0.3690 - acc: 0.8572 - val_loss: 0.2540 - val_acc: 0.9105\n",
            "\n",
            "Epoch 2/5\n",
            "313/313 - 1s - loss: 0.2476 - acc: 0.8986 - val_loss: 0.1672 - val_acc: 0.9227\n",
            "\n",
            "Epoch 3/5\n",
            "313/313 - 1s - loss: 0.1906 - acc: 0.9163 - val_loss: 0.1596 - val_acc: 0.9280\n",
            "\n",
            "Epoch 4/5\n",
            "313/313 - 1s - loss: 0.1612 - acc: 0.9321 - val_loss: 0.1318 - val_acc: 0.9299\n",
            "\n",
            "Epoch 5/5\n",
            "313/313 - 1s - loss: 0.1450 - acc: 0.9391 - val_loss: 0.1137 - val_acc: 0.9487\n",
            "\n",
            "start predict\n",
            "(900000, 10, 10, 1)\n",
            "(900000,)\n",
            "16000\n",
            "4000\n",
            "180000\n",
            "720000\n",
            "ok\n",
            "new saved model:{'balanced_accuracy_val': 90.0125, 'TP_val': 15696, 'FN_val': 304, 'FP_val': 723, 'TN_val': 3277, 'balanced_accuracy_test': 89.41791666666667, 'TN_test': 707526, 'FP_test': 12474, 'FN_test': 34977, 'TP_test': 145023, 'kernel': 3, 'learning_rate': 0.000548102302785107, 'batch': 256, 'time': '00:00:29', 'OA_val': 0.94865, 'P_val': 0.9559656495523479, 'R_val': 0.981, 'F1_val': 0.9683210463000093, 'FAR_val': 0.18075, 'TPR_val': 0.981, 'OA_test': 0.9472766666666667, 'P_test': 0.9207984913998362, 'R_test': 0.8056833333333333, 'F1_test': 0.859403194695064, 'FAR_test': 0.017325, 'TPR_test': 0.8056833333333333}\n",
            "prova0.9683210463000093\n",
            "start train\n",
            "{'batch': 64, 'dropout1': 0.25708316586622415, 'dropout2': 0.20751326510863233, 'epoch': 5, 'kernel': 4, 'learning_rate': 0.0002618274071341585}\n",
            "Epoch 1/5\n",
            "1250/1250 - 4s - loss: 0.3100 - acc: 0.8776 - val_loss: 0.1931 - val_acc: 0.9153\n",
            "\n",
            "Epoch 2/5\n",
            "1250/1250 - 4s - loss: 0.1934 - acc: 0.9184 - val_loss: 0.1380 - val_acc: 0.9467\n",
            "\n",
            "Epoch 3/5\n",
            "1250/1250 - 4s - loss: 0.1554 - acc: 0.9326 - val_loss: 0.1152 - val_acc: 0.9499\n",
            "\n",
            "Epoch 4/5\n",
            "1250/1250 - 4s - loss: 0.1354 - acc: 0.9420 - val_loss: 0.1057 - val_acc: 0.9582\n",
            "\n",
            "Epoch 5/5\n",
            "1250/1250 - 4s - loss: 0.1220 - acc: 0.9484 - val_loss: 0.1006 - val_acc: 0.9607\n",
            "\n",
            "start predict\n",
            "(900000, 10, 10, 1)\n",
            "(900000,)\n",
            "16000\n",
            "4000\n",
            "180000\n",
            "720000\n",
            "ok\n",
            "new saved model:{'balanced_accuracy_val': 94.45, 'TP_val': 15544, 'FN_val': 456, 'FP_val': 330, 'TN_val': 3670, 'balanced_accuracy_test': 94.05250000000001, 'TN_test': 699696, 'FP_test': 20304, 'FN_test': 16335, 'TP_test': 163665, 'kernel': 4, 'learning_rate': 0.0002618274071341585, 'batch': 64, 'time': '00:00:41', 'OA_val': 0.9607, 'P_val': 0.9792112889000882, 'R_val': 0.9715, 'F1_val': 0.9753404028361674, 'FAR_val': 0.0825, 'TPR_val': 0.9715, 'OA_test': 0.95929, 'P_test': 0.8896335795704711, 'R_test': 0.90925, 'F1_test': 0.8993348334610914, 'FAR_test': 0.0282, 'TPR_test': 0.90925}\n",
            "prova0.9753404028361674\n",
            "start train\n",
            "{'batch': 128, 'dropout1': 0.6950762980305488, 'dropout2': 0.5136897780317312, 'epoch': 5, 'kernel': 3, 'learning_rate': 0.0005493916540323797}\n",
            "Epoch 1/5\n",
            "625/625 - 2s - loss: 0.3888 - acc: 0.8531 - val_loss: 0.3006 - val_acc: 0.9067\n",
            "\n",
            "Epoch 2/5\n",
            " 20%|██        | 4/20 [02:51<11:25, 42.87s/it, best loss: -0.8993348334610914]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-51d0a9dc2db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m           \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Ytest\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_myfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m           \u001b[0mf_myfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-0262687530f8>\u001b[0m in \u001b[0;36mtrain_norm\u001b[0;34m(param, dataset, norm)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mattack_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attack_label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperopt_fcn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizable_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hyper_opt_evals\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-0262687530f8>\u001b[0m in \u001b[0;36mhyperopt_fcn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_Nature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXGlobal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYGlobal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mMode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"CNN2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXGlobal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYGlobal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start predict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXTestGlobal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-950c19a3102a>\u001b[0m in \u001b[0;36mCNN2\u001b[0;34m(images, y, params)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),\n\u001b[0;32m--> 168\u001b[0;31m                    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\u001b[0m\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASUmNndnbHXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeUP7VXWljqf",
        "colab_type": "code",
        "outputId": "1bafc523-fc08-4f8d-c5d0-91082b3ed1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "imagaes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-630b579f703f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimagaes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'imagaes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjwqBn5hQDXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('Original dataset/Testing.csv', 'r') as file:\n",
        "        Xtest = pd.DataFrame(list(csv.DictReader(file)))\n",
        "        #Xtest.drop(Xtest.keys()[0], axis=1)\n",
        "        Xtest.replace(\"\", np.nan, inplace=True)\n",
        "        Xtest.dropna(inplace=True)\n",
        "        data[\"Xtest\"] = Xtest[Xtest.keys()[1:]].astype(float)\n",
        "        data[\"Ytest\"] = data[\"Xtest\"]['Classification']\n",
        "        del data[\"Xtest\"]['Classification']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYe43rJ3uWER",
        "colab_type": "code",
        "outputId": "177f1c60-d4da-4295-dd77-2dd2a2c50cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data[\"Xtest\"].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(900000, 78)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}