{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tesi 3.1 Autoencoder 8.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "d3-ITpNnp9vR"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxyxHdTRcI00xcnpjR9MQs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kyanji/THESIS_WORK_malware-detection_through_images_and_gan/blob/master/Tesi_3_2_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar0YsN0Qy4NZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2by81Ycp1Ke",
        "colab_type": "text"
      },
      "source": [
        "# Cart2Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4dO5a_ouRjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def minimum_bounding_rectangle(points):\n",
        "    \"\"\"\n",
        "    Find the smallest bounding rectangle for a set of points.\n",
        "    Returns a set of points representing the corners of the bounding box.\n",
        "\n",
        "    :param points: an nx2 matrix of coordinates\n",
        "    :rval: an nx2 matrix of coordinates\n",
        "    \"\"\"\n",
        "    from scipy.ndimage.interpolation import rotate\n",
        "    pi2 = np.pi/2.\n",
        "\n",
        "    # get the convex hull for the points\n",
        "    hull_points = points[ConvexHull(points).vertices]\n",
        "\n",
        "    # calculate edge angles\n",
        "    edges = np.zeros((len(hull_points)-1, 2))\n",
        "    edges = hull_points[1:] - hull_points[:-1]\n",
        "\n",
        "    angles = np.zeros((len(edges)))\n",
        "    angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
        "\n",
        "    angles = np.abs(np.mod(angles, pi2))\n",
        "    angles = np.unique(angles)\n",
        "\n",
        "    # find rotation matrices\n",
        "    # XXX both work\n",
        "    rotations = np.vstack([\n",
        "        np.cos(angles),\n",
        "        np.cos(angles-pi2),\n",
        "        np.cos(angles+pi2),\n",
        "        np.cos(angles)]).T\n",
        "#     rotations = np.vstack([\n",
        "#         np.cos(angles),\n",
        "#         -np.sin(angles),\n",
        "#         np.sin(angles),\n",
        "#         np.cos(angles)]).T\n",
        "    rotations = rotations.reshape((-1, 2, 2))\n",
        "\n",
        "    # apply rotations to the hull\n",
        "    rot_points = np.dot(rotations, hull_points.T)\n",
        "\n",
        "    # find the bounding points\n",
        "    min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
        "    max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
        "    min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
        "    max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
        "\n",
        "    # find the box with the best area\n",
        "    areas = (max_x - min_x) * (max_y - min_y)\n",
        "    best_idx = np.argmin(areas)\n",
        "\n",
        "    # return the best box\n",
        "    x1 = max_x[best_idx]\n",
        "    x2 = min_x[best_idx]\n",
        "    y1 = max_y[best_idx]\n",
        "    y2 = min_y[best_idx]\n",
        "    r = rotations[best_idx]\n",
        "\n",
        "    rval = np.zeros((4, 2))\n",
        "    rval[0] = np.dot([x1, y2], r)\n",
        "    rval[1] = np.dot([x2, y2], r)\n",
        "    rval[2] = np.dot([x2, y1], r)\n",
        "    rval[3] = np.dot([x1, y1], r)\n",
        "\n",
        "    return rval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8ETXLqco-R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "\n",
        "def ConvPixel(FVec, xp, yp, A, B, base=1, index=0):\n",
        "    n = len(FVec)\n",
        "    M = np.ones([int(A), int(B)]) * base\n",
        "    for j in range(0, n):\n",
        "        # M[int(xp[j]) - 1, int(yp[j]) - 1] = 0\n",
        "        M[int(xp[j]) - 1, int(yp[j]) - 1] = FVec[j]\n",
        "    zp = np.array([xp, yp])\n",
        "\n",
        "    # zp[:, 0] = zp[:, 12]\n",
        "    # zp[:, 13] = zp[:, 0]\n",
        "    # zp[:, 15] = zp[:, 0]\n",
        "    #\n",
        "    # zp[:,6] = zp[:, 5]\n",
        "    # zp[:, 2] = zp[:, 6]\n",
        "    # zp[:, 11] = zp[:, 6]\n",
        "\n",
        "    dup = {}\n",
        "    # find duplicate\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                # if i in dup.keys():\n",
        "                # print(\"duplicate:\" + str(i) + \" \" + str(j) + \"value: \")\n",
        "                # dup.add(i)\n",
        "                # dup[i].add(j)\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "                # print(\"Collisione\")\n",
        "\n",
        "    # print(\"Collisioni:\")\n",
        "    # print(dup.keys())\n",
        "\n",
        "    for index in dup.keys():\n",
        "        x, y = index.split(\"-\")\n",
        "        M[int(float(x)) - 1, int(float(y)) - 1] = sum(FVec[list(dup[index])]) / len(dup[index])\n",
        "\n",
        "    return M\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpIg0smSpxaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import pickle\n",
        "\n",
        "import pandas as pd\n",
        "import json as json\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def find_duplicate(zp):\n",
        "    dup = {}\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "    sum = 0\n",
        "    for ind in dup.keys():\n",
        "        sum = sum + (len(dup[ind]) - 1)\n",
        "    return sum\n",
        "\n",
        "\n",
        "def dataset_with_best_duplicates(X, y, zp):\n",
        "    X = X.transpose()\n",
        "    dup = {}\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if int(zp[0, i]) == int(zp[0, j]) and int(zp[1, i]) == int(zp[1, j]):\n",
        "                dup.setdefault(str(zp[0, i]) + \"-\" + str(zp[1, i]), {i}).add(j)\n",
        "\n",
        "    # print(\"Collisioni:\" + str(len(dup.keys())))\n",
        "    # print(dup.keys())\n",
        "    toDelete = []\n",
        "    for index in dup.keys():\n",
        "        mi = []\n",
        "        x_new = X[:, list(dup[index])]\n",
        "        mi = mutual_info_classif(x_new, y)\n",
        "        max = np.argmax(mi)\n",
        "        dup[index].remove(list(dup[index])[max])\n",
        "        toDelete.extend(list(dup[index]))\n",
        "    X = np.delete(X, toDelete, axis=1)\n",
        "    zp = np.delete(zp, toDelete, axis=1)\n",
        "    return X.transpose(), zp, toDelete\n",
        "\n",
        "def count_model_col(rotatedData,Q,r1,r2):\n",
        "    tot = []\n",
        "    for f in range(r1-1, r2):\n",
        "        A = f\n",
        "        B = f\n",
        "        xp = np.round(\n",
        "            1 + (A * (rotatedData[0, :] - min(rotatedData[0, :])) / (max(rotatedData[0, :]) - min(rotatedData[0, :]))))\n",
        "        yp = np.round(\n",
        "            1 + (-B) * (rotatedData[1, :] - max(rotatedData[1, :])) / (max(rotatedData[1, :]) - min(rotatedData[1, :])))\n",
        "        zp = np.array([xp, yp])\n",
        "        A = max(xp)\n",
        "        B = max(yp)\n",
        "\n",
        "        # find duplicates\n",
        "        sum=str(find_duplicate(zp))\n",
        "        print(\"Collisioni: \" + sum)\n",
        "        tot.append([A,sum])\n",
        "        a = ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B)\n",
        "        plt.imshow(a, cmap=\"gray\")\n",
        "        plt.savefig(str(A)+'.png')\n",
        "        plt.show()\n",
        "    pd.DataFrame(tot).to_csv(\"Collision_autoencoder.csv\")\n",
        "\n",
        "\n",
        "def Cart2Pixel(Q=None, A=None, B=None, dynamic_size=False, mutual_info=False, only_model=False, params=None):\n",
        "    # TODO controls on input\n",
        "    if A is not None:\n",
        "        A = A - 1\n",
        "    if (B != None):\n",
        "        B = B - 1\n",
        "    # to dataframe\n",
        "    feat_cols = [\"col-\" + str(i + 1) for i in range(Q[\"data\"].shape[1])]\n",
        "    df = pd.DataFrame(Q[\"data\"], columns=feat_cols)\n",
        "    if Q[\"method\"] == 'pca':\n",
        "        pca = PCA(n_components=2)\n",
        "        Y = pca.fit_transform(df)\n",
        "    elif Q[\"method\"] == 'tSNE':\n",
        "        tsne = TSNE(n_components=2, method=\"exact\")\n",
        "        Y = tsne.fit_transform(df)\n",
        "    elif Q[\"method\"] == 'kpca':\n",
        "        kpca = KernelPCA(n_components=2, kernel='linear')\n",
        "        Y = kpca.fit_transform(df)\n",
        "\n",
        "    x = Y[:, 0]\n",
        "    y = Y[:, 1]\n",
        "    n, n_sample = Q[\"data\"].shape\n",
        "    # plt.scatter(x, y)\n",
        "    bbox = minimum_bounding_rectangle(Y)\n",
        "    # plt.fill(bbox[:, 0], bbox[:, 1], alpha=0.2)\n",
        "    # rotation\n",
        "    grad = (bbox[1, 1] - bbox[0, 1]) / (bbox[1, 0] - bbox[0, 0])\n",
        "    theta = np.arctan(grad)\n",
        "    R = np.asmatrix([[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]])\n",
        "    bboxMatrix = np.matrix(bbox)\n",
        "    zrect = (R.dot(bboxMatrix.transpose())).transpose()\n",
        "    # zrect=R.dot(bboxMatrix)\n",
        "    # plt.fill(zrect[:, 0], zrect[:, 1], alpha=0.2)\n",
        "\n",
        "    coord = np.array([x, y])\n",
        "    rotatedData = np.array(R.dot(coord))  # Z\n",
        "\n",
        "    # rotatedData = np.delete(rotatedData, [125], 1)\n",
        "    # rotatedData=np.delete(rotatedData, [175],1)\n",
        "    # rotatedData=np.delete(rotatedData, [184],1)\n",
        "    # Q[\"data\"] = np.delete(Q[\"data\"], [125], axis=0)\n",
        "    # Q[\"data\"] = np.delete(Q[\"data\"], [175], axis=0)\n",
        "    # Q[\"data\"] = np.delete(Q[\"data\"], [184], axis=0)\n",
        "    # n = n - 3\n",
        "    plt.scatter(rotatedData[0, :], rotatedData[1:])\n",
        "    plt.axis('square')\n",
        "    plt.show(block=False)\n",
        "\n",
        "    # find duplicate\n",
        "    for i in range(len(rotatedData[0, :])):\n",
        "        for j in range(i + 1, len(rotatedData[0])):\n",
        "            if rotatedData[0, i] == rotatedData[0, j] and rotatedData[1, i] == rotatedData[1, j]:\n",
        "                print(\"duplicate:\" + str(i) + \" \" + str(j))\n",
        "\n",
        "    # nearest point\n",
        "\n",
        "    min_dist = np.inf\n",
        "    min_p1 = 0\n",
        "    min_p2 = 0\n",
        "    for p1 in range(n):\n",
        "        for p2 in range(p1 + 1, n):\n",
        "            d = (rotatedData[0, p1] - rotatedData[0, p2]) ** 2 + (rotatedData[1, p1] - rotatedData[1, p2]) ** 2\n",
        "            if min_dist > d > 0 and p1 != p2:\n",
        "                min_p1 = p1\n",
        "                min_p2 = p2\n",
        "                min_dist = d\n",
        "    # plt.scatter([rotatedData[0, min_p1], rotatedData[0, min_p2]], [rotatedData[1, min_p1], rotatedData[1, min_p2]])\n",
        "    # plt.show(block=False)\n",
        "\n",
        "    # euclidean distance\n",
        "    dmin = np.linalg.norm(rotatedData[:, min_p1] - rotatedData[:, min_p2])\n",
        "    rec_x_axis = abs(zrect[0, 0] - zrect[1, 0])\n",
        "    rec_y_axis = abs(zrect[1, 1] - zrect[2, 1])\n",
        "\n",
        "    #count_model_col(rotatedData,Q,5,20)\n",
        "\n",
        "    if dynamic_size:\n",
        "        precision_old = math.sqrt(2)\n",
        "        A = math.ceil(rec_x_axis * precision_old / dmin)\n",
        "        B = math.ceil(rec_y_axis * precision_old / dmin)\n",
        "        print(\"Dynamic [A:\" + str(A) + \" ; B:\" + str(B) + \"]\")\n",
        "        if A > Q[\"max_A_size\"] or B > Q[\"max_B_size\"]:\n",
        "            # precision = precision_old * Q[\"max_px_size\"] / max([A, B])\n",
        "            precision = precision_old * (Q[\"max_A_size\"] / A) * (Q[\"max_B_size\"] / B)\n",
        "            A = math.ceil(rec_x_axis * precision / dmin)\n",
        "            B = math.ceil(rec_y_axis * precision / dmin)\n",
        "    # cartesian coordinates to pixels\n",
        "    tot = []\n",
        "    xp = np.round(\n",
        "        1 + (A * (rotatedData[0, :] - min(rotatedData[0, :])) / (max(rotatedData[0, :]) - min(rotatedData[0, :]))))\n",
        "    yp = np.round(\n",
        "        1 + (-B) * (rotatedData[1, :] - max(rotatedData[1, :])) / (max(rotatedData[1, :]) - min(rotatedData[1, :])))\n",
        "    zp = np.array([xp, yp])\n",
        "    A = max(xp)\n",
        "    B = max(yp)\n",
        "\n",
        "    # find duplicates\n",
        "    print(\"Collisioni: \" + str(find_duplicate(zp)))\n",
        "\n",
        "\n",
        "    # Training set\n",
        "\n",
        "    images = []\n",
        "    toDelete = 0\n",
        "    name = \"_\" + str(int(A)) + 'x' + str(int(B))\n",
        "    if params[\"No_0_MI\"]:\n",
        "        name = name + \"_No_0_MI\"\n",
        "    if mutual_info:\n",
        "        Q[\"data\"], zp, toDelete = dataset_with_best_duplicates(Q[\"data\"], Q[\"y\"], zp)\n",
        "        name = name + \"_MI\"\n",
        "    else:\n",
        "        name = name + \"_Mean\"\n",
        "    if only_model:\n",
        "        a = ConvPixel(Q[\"data\"][:, 0], zp[0], zp[1], A, B)\n",
        "        plt.imshow(a, cmap=\"gray\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        images = [ConvPixel(Q[\"data\"][:, i], zp[0], zp[1], A, B, index=i) for i in range(0, n_sample)]\n",
        "        filename = params[\"dir\"] + \"train\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(images, f_myfile)\n",
        "        f_myfile.close()\n",
        "        filename = params[\"res\"] + \"train\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(images, f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "    image_model = {\"xp\": zp[0].tolist(), \"yp\": zp[1].tolist(), \"A\": A, \"B\": B}\n",
        "    j = json.dumps(image_model)\n",
        "    f = open(params[\"dir\"] + name + \"model.json\", \"w\")\n",
        "    f.write(j)\n",
        "    f.close()\n",
        "\n",
        "    return images, image_model, toDelete\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3-ITpNnp9vR",
        "colab_type": "text"
      },
      "source": [
        "# Neural"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTVqqtzop6T5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Activation, AveragePooling2D, Add, \\\n",
        "    Concatenate, Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def CNN_Nature(images, y, param=None):\n",
        "    print(param)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        stratify=y,\n",
        "                                                        random_state=100)\n",
        "    x_train = np.array(x_train)\n",
        "    x_test = np.array(x_test)\n",
        "\n",
        "    image_size = x_train.shape[1]\n",
        "    image_size2 = x_train.shape[2]\n",
        "\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size2, 1])\n",
        "    x_test = np.reshape(x_test, [-1, image_size, image_size2, 1])\n",
        "\n",
        "    num_filters = param[\"filter\"]\n",
        "    num_filters2 = param[\"filter2\"]\n",
        "\n",
        "    kernel = param[\"kernel\"]\n",
        "\n",
        "    inputs = Input(shape=(image_size, image_size2, 1))\n",
        "    print(x_train.shape)\n",
        "    out = Conv2D(filters=num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(inputs)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=2 * num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=4 * num_filters,\n",
        "                 kernel_size=(kernel, kernel),\n",
        "                 padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "\n",
        "    # layer 2\n",
        "    out2 = Conv2D(filters=num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(inputs)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=2 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=4 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "\n",
        "    # final layer\n",
        "    outf = Concatenate()([out, out2])\n",
        "    out_f = AveragePooling2D(strides=2, pool_size=2)(outf)\n",
        "    out_f = Flatten()(out_f)\n",
        "    predictions = Dense(2, activation='softmax')(out_f)\n",
        "\n",
        "    # This creates a model that includes\n",
        "    # the Input layer and three Dense layers\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "    adam = Adam(lr=param[\"learning_rate\"])\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=adam,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "\n",
        "    # Train the model.\n",
        "    hist = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=param[\"epoch\"],\n",
        "        verbose=2,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=param[\"batch\"],\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),\n",
        "                   ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "    )\n",
        "    model.load_weights('best_model.h5')\n",
        "\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    Y_predicted = model.predict(x_test, verbose=0, use_multiprocessing=True, workers=12)\n",
        "\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "\n",
        "    cf = confusion_matrix(y_test, Y_predicted)\n",
        "\n",
        "    return model, {\"balanced_accuracy_val\": balanced_accuracy_score(y_test, Y_predicted) * 100, \"TP_val\": cf[0][0],\n",
        "                   \"FN_val\": cf[0][1], \"FP_val\": cf[1][0], \"TN_val\": cf[1][1]\n",
        "                   }\n",
        "\n",
        "\n",
        "def CNN2(images, y, params=None):\n",
        "    print(params)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        stratify=y,\n",
        "                                                        random_state=100\n",
        "                                                        )\n",
        "    x_train = np.array(x_train)\n",
        "    x_test = np.array(x_test)\n",
        "\n",
        "    image_size = x_train.shape[1]\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "    x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "\n",
        "    num_filters = params[\"filter\"]\n",
        "    num_filters2 = params[\"filter2\"]\n",
        "\n",
        "    kernel = params[\"kernel\"]\n",
        "\n",
        "    inputs = Input(shape=(image_size, x_train.shape[2], 1))\n",
        "\n",
        "    X = Conv2D(32, (2, 2), activation='relu', name='conv0')(inputs)\n",
        "    X = Dropout(rate=params['dropout1'])(X)\n",
        "    X = Conv2D(64, (2, 2), activation='relu', name='conv1')(X)\n",
        "    X = Dropout(rate=params['dropout2'])(X)\n",
        "    X = Conv2D(128, (1, 2), activation='relu', name='conv2')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(256, activation='relu', kernel_initializer='glorot_uniform')(X)\n",
        "    X = Dense(1024, activation='relu', kernel_initializer='glorot_uniform')(X)\n",
        "    X = Dense(2, activation='softmax', kernel_initializer='glorot_uniform')(X)\n",
        "\n",
        "    model = Model(input=inputs, output=X)\n",
        "    adam = Adam(params[\"learning_rate\"])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=adam,\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    # Train the model.\n",
        "    hist = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=params[\"epoch\"],\n",
        "        verbose=2,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=params[\"batch\"],\n",
        "        callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=10),\n",
        "                   ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "    )\n",
        "    model.load_weights('best_model.h5')\n",
        "\n",
        "    y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "    Y_predicted = model.predict(x_test, verbose=0, use_multiprocessing=True, workers=12)\n",
        "\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "\n",
        "    cf = confusion_matrix(y_test, Y_predicted)\n",
        "\n",
        "    return model, {\"balanced_accuracy_val\": balanced_accuracy_score(y_test, Y_predicted) * 100, \"TP_val\": cf[0][0],\n",
        "                   \"FN_val\": cf[0][1], \"FP_val\": cf[1][0], \"TN_val\": cf[1][1]\n",
        "                   }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCBvyo-1uWzy",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFBUf1WUuaNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "from hyperopt import STATUS_OK\n",
        "from hyperopt import tpe, hp, Trials, fmin\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "\n",
        "import time\n",
        "\n",
        "XGlobal = []\n",
        "YGlobal = []\n",
        "\n",
        "XTestGlobal = []\n",
        "YTestGlobal = []\n",
        "\n",
        "SavedParameters = []\n",
        "Mode = \"\"\n",
        "Name = \"\"\n",
        "best_val_acc = 0\n",
        "path_model=\"\"\n",
        "\n",
        "\n",
        "def res(cm, val):\n",
        "    tp = cm[0][0]  # attacks true\n",
        "    fn = cm[0][1]  # attacs predict normal\n",
        "    fp = cm[1][0]  # normal predict attacks\n",
        "    tn = cm[1][1]  # normal as normal\n",
        "    attacks = tp + fn\n",
        "    normals = fp + tn\n",
        "    if val and normals == 7400:\n",
        "        print(\"ok\")\n",
        "    elif val:\n",
        "        print(\"error val\")\n",
        "    if (not val) and normals == 56000:\n",
        "        print(\"ok\")\n",
        "    elif not val:\n",
        "        print(\"error\")\n",
        "    OA = (tp + tn) / (attacks + normals)\n",
        "    AA = ((tp / attacks) + (tn / normals)) / 2\n",
        "    P = tp / (tp + fp)\n",
        "    R = tp / (tp + fn)\n",
        "    F1 = 2 * ((P * R) / (P + R))\n",
        "    FAR = fp / (fp + tn)\n",
        "    TPR = tp / (tp + fn)\n",
        "    r = [OA, AA, P, R, F1, FAR, TPR]\n",
        "    return r\n",
        "\n",
        "def hyperopt_fcn(params):\n",
        "    if params[\"filter\"] == params[\"filter2\"]:\n",
        "        return {'loss': np.inf, 'status': STATUS_OK}\n",
        "    global SavedParameters\n",
        "    start_time = time.time()\n",
        "    print(\"start train\")\n",
        "    if Mode == \"CNN_Nature\":\n",
        "        model, val = CNN_Nature(XGlobal, YGlobal, params)\n",
        "    elif Mode == \"CNN2\":\n",
        "        model, val = CNN2(XGlobal, YGlobal, params)\n",
        "    print(\"start predict\")\n",
        "\n",
        "    Y_predicted = model.predict(XTestGlobal, verbose=0, use_multiprocessing=True, workers=12)\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    cf = confusion_matrix(YTestGlobal, Y_predicted)\n",
        "    #print(cf)\n",
        "    #print(\"test accuracy: \"+str(balanced_accuracy_score(YTestGlobal, Y_predicted)))\n",
        "    K.clear_session()\n",
        "    SavedParameters.append(val)\n",
        "    global best_val_acc\n",
        "    #print(\"val acc: \"+str(val[\"balanced_accuracy_val\"]))\n",
        "    \n",
        "    if Mode == \"CNN_Nature\":\n",
        "        SavedParameters[-1].update({\"balanced_accuracy_test\": balanced_accuracy_score(YTestGlobal, Y_predicted) *\n",
        "                                                              100, \"TP_test\": cf[0][0], \"FN_test\": cf[0][1],\n",
        "                                    \"FP_test\": cf[1][0], \"TN_test\": cf[1][1], \"kernel\": params[\n",
        "                \"kernel\"], \"learning_rate\": params[\"learning_rate\"], \"batch\": params[\"batch\"],\n",
        "                                    \"filter1\": params[\"filter\"],\n",
        "                                    \"filter2\": params[\"filter2\"],\n",
        "                                    \"time\": time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))})\n",
        "    elif Mode == \"CNN2\":\n",
        "        SavedParameters[-1].update(\n",
        "            {\"balanced_accuracy_test\": balanced_accuracy_score(YTestGlobal, Y_predicted) * 100, \"TN_test\": cf[0][0],\n",
        "             \"FP_test\": cf[0][1], \"FN_test\": cf[1][0], \"TP_test\": cf[1][1], \"kernel\": params[\"kernel\"],\n",
        "             \"learning_rate\": params[\"learning_rate\"],\n",
        "             \"batch\": params[\"batch\"],\n",
        "             \"filter1\": params[\"filter\"],\n",
        "             \"filter2\": params[\"filter2\"],\n",
        "             \"time\": time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))})\n",
        "    cm_val = [[SavedParameters[-1][\"TP_val\"], SavedParameters[-1][\"FN_val\"]],\n",
        "              [SavedParameters[-1][\"FP_val\"], SavedParameters[-1][\"TN_val\"]]]\n",
        "\n",
        "    r = res(cm_val, True)\n",
        "    SavedParameters[-1].update({\n",
        "        \"OA_val\": r[0],\n",
        "        \"P_val\": r[2],\n",
        "        \"R_val\": r[3],\n",
        "        \"F1_val\": r[4],\n",
        "        \"FAR_val\": r[5],\n",
        "        \"TPR_val\": r[6]\n",
        "    })\n",
        "    cm_test = [[SavedParameters[-1][\"TP_test\"], SavedParameters[-1][\"FN_test\"]], \n",
        "               [SavedParameters[-1][\"FP_test\"], SavedParameters[-1][\"TN_test\"]]]\n",
        "    r = res(cm_test, False)\n",
        "    SavedParameters[-1].update({\n",
        "        \"OA_test\": r[0],\n",
        "        \"P_test\": r[2],\n",
        "        \"R_test\": r[3],\n",
        "        \"F1_test\": r[4],\n",
        "        \"FAR_test\": r[5],\n",
        "        \"TPR_test\": r[6]\n",
        "    })\n",
        "    #Save Model\n",
        "    if SavedParameters[-1][\"F1_val\"] > best_val_acc:\n",
        "        print(\"new saved model:\" + str(SavedParameters[-1]))\n",
        "        model.save(path_model)\n",
        "        best_val_acc = SavedParameters[-1][\"F1_val\"]\n",
        "\n",
        "\n",
        "    SavedParameters = sorted(SavedParameters, key=lambda i: i['F1_val'], reverse=True)\n",
        "\n",
        "    try:\n",
        "        with open(Name, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=SavedParameters[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(SavedParameters)\n",
        "    except IOError:\n",
        "        print(\"I/O error\")\n",
        "    print(\"prova\"+str(val[\"F1_val\"]))\n",
        "    return {'loss': -val[\"F1_val\"], 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def train_norm(param, dataset, norm):\n",
        "    np.random.seed(param[\"seed\"])\n",
        "    print(\"modelling dataset\")\n",
        "    global YGlobal\n",
        "    YGlobal = to_categorical(dataset[\"Classification\"])\n",
        "    del dataset[\"Classification\"]\n",
        "    global YTestGlobal\n",
        "    YTestGlobal = to_categorical(dataset[\"Ytest\"])\n",
        "    del dataset[\"Ytest\"]\n",
        "\n",
        "    global XGlobal\n",
        "    global XTestGlobal\n",
        "\n",
        "    if not param[\"LoadFromJson\"]:\n",
        "        # norm\n",
        "        Out = {}\n",
        "        if norm:\n",
        "            print('NORM Min-Max')\n",
        "            Out[\"Max\"] = float(dataset[\"Xtrain\"].max().max())\n",
        "            Out[\"Min\"] = float(dataset[\"Xtrain\"].min().min())\n",
        "            # NORM\n",
        "            dataset[\"Xtrain\"] = (dataset[\"Xtrain\"] - Out[\"Min\"]) / (Out[\"Max\"] - Out[\"Min\"])\n",
        "            dataset[\"Xtrain\"] = dataset[\"Xtrain\"].fillna(0)\n",
        "\n",
        "        # TODO implement norm 2\n",
        "        print(\"trasposing\")\n",
        "\n",
        "        q = {\"data\": np.array(dataset[\"Xtrain\"].values).transpose(), \"method\": param[\"Metod\"],\n",
        "             \"max_A_size\": param[\"Max_A_Size\"], \"max_B_size\": param[\"Max_B_Size\"], \"y\": np.argmax(YGlobal, axis=1)}\n",
        "        print(q[\"method\"])\n",
        "        print(q[\"max_A_size\"])\n",
        "        print(q[\"max_B_size\"])\n",
        "\n",
        "        # generate images\n",
        "        XGlobal, image_model, toDelete = Cart2Pixel(q, q[\"max_A_size\"], q[\"max_B_size\"], param[\"Dynamic_Size\"],\n",
        "                                                    mutual_info=param[\"mutual_info\"], params=param)\n",
        "\n",
        "        del q[\"data\"]\n",
        "        print(\"Train Images done!\")\n",
        "        # generate testingset image\n",
        "        if param[\"mutual_info\"]:\n",
        "            dataset[\"Xtest\"] = dataset[\"Xtest\"].drop(dataset[\"Xtest\"].columns[toDelete], axis=1)\n",
        "\n",
        "        dataset[\"Xtest\"] = np.array(dataset[\"Xtest\"]).transpose()\n",
        "        print(\"generating Test Images\")\n",
        "        print(dataset[\"Xtest\"].shape)\n",
        "        XTestGlobal = [ConvPixel(dataset[\"Xtest\"][:, i], np.array(image_model[\"xp\"]), np.array(image_model[\"yp\"]),\n",
        "                                 image_model[\"A\"], image_model[\"B\"]) for i in\n",
        "                       range(0, dataset[\"Xtest\"].shape[1])]  # dataset[\"Xtest\"].shape[1])]\n",
        "        print(\"Test Images done!\")\n",
        "\n",
        "        # saving testingset\n",
        "        name = \"_\" + str(int(q[\"max_A_size\"])) + \"x\" + str(int(q[\"max_B_size\"]))\n",
        "        if param[\"No_0_MI\"]:\n",
        "            name = name + \"_No_0_MI\"\n",
        "        if param[\"mutual_info\"]:\n",
        "            name = name + \"_MI\"\n",
        "        else:\n",
        "            name = name + \"_Mean\"\n",
        "        filename = param[\"dir\"] + \"test\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(XTestGlobal, f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "        filename = param[\"res\"] + \"test\" + name + \".pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(XTestGlobal, f_myfile)\n",
        "        f_myfile.close()\n",
        "    else:\n",
        "        XGlobal = dataset[\"Xtrain\"]\n",
        "        XTestGlobal = dataset[\"Xtest\"]\n",
        "    del dataset[\"Xtrain\"]\n",
        "    del dataset[\"Xtest\"]\n",
        "    XTestGlobal = np.array(XTestGlobal)\n",
        "    image_size = XTestGlobal.shape[1]\n",
        "    print(\"shape\" + str(XTestGlobal.shape))\n",
        "    XTestGlobal = np.reshape(XTestGlobal, [-1, image_size, image_size, 1])\n",
        "    YTestGlobal = np.argmax(YTestGlobal, axis=1)\n",
        "\n",
        "    # optimizable_variable = {\"filter_size\": 3, \"kernel\": 2, \"filter_size2\": 6,\"learning_rate\":1e-5,\"momentum\":0.8}\n",
        "\n",
        "    if param[\"Mode\"] == \"CNN_Nature\":\n",
        "        optimizable_variable = {\"kernel\": hp.choice(\"kernel\", np.arange(2, 7 + 1)),\n",
        "                                \"filter\": hp.choice(\"filter\", [16, 32, 64, 128]),\n",
        "                                \"filter2\": hp.choice(\"filter2\", [16, 32, 64, 128]),\n",
        "                                \"batch\": hp.choice(\"batch\", [32]),\n",
        "                                \"learning_rate\": hp.uniform(\"learning_rate\", 0.0001, 0.01),\n",
        "                                \"epoch\": param[\"epoch\"]}\n",
        "    elif param[\"Mode\"] == \"CNN2\":\n",
        "        optimizable_variable = {\"kernel\": hp.choice(\"kernel\", np.arange(2, 7 + 1)),\n",
        "            \"filter\": hp.choice(\"filter\", [16, 32, 64, 128]),\n",
        "            \"filter2\": hp.choice(\"filter2\", [16, 32, 64, 128]),\n",
        "            \"batch\": hp.choice(\"batch\", [32, 64, 128, 256, 512]),\n",
        "            'dropout1': hp.uniform(\"dropout1\", 0, 1),\n",
        "            'dropout2': hp.uniform(\"dropout2\", 0, 1),\n",
        "            \"learning_rate\": hp.uniform(\"learning_rate\", 1e-4, 1e-1),\n",
        "            \"epoch\": param[\"epoch\"]}\n",
        "    global Mode\n",
        "    Mode = param[\"Mode\"]\n",
        "\n",
        "    global Name\n",
        "    Name = param[\"res\"] + \"res_\" + str(int(param[\"Max_A_Size\"])) + \"x\" + str(int(param[\"Max_B_Size\"]))\n",
        "    if param[\"No_0_MI\"]:\n",
        "        Name = Name + \"_No_0_MI\"\n",
        "    if param[\"mutual_info\"]:\n",
        "        Name = Name + \"_MI\"\n",
        "    else:\n",
        "        Name = Name + \"_Mean\"\n",
        "    Name = Name + \"_\" + Mode + \".csv\"\n",
        "    \n",
        "    global path_model\n",
        "    path_model=Name.replace(\".csv\",\".h5\")\n",
        "\n",
        "    trials = Trials()\n",
        "    fmin(hyperopt_fcn, optimizable_variable, trials=trials, algo=tpe.suggest, max_evals=param[\"hyper_opt_evals\"])\n",
        "\n",
        "    print(\"done\")\n",
        "    return 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlrDyDGkuerB",
        "colab_type": "text"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5dXr--PuhcM",
        "colab_type": "code",
        "outputId": "dc312075-79bd-4cbe-e317-2790fdf8af9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r \"/content/drive/My Drive/dataset2/res/8/\" /content #path for the dataset or pickle files\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTtzoSZvuoUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Parameters\n",
        "param = {\"Max_A_Size\": 8, \"Max_B_Size\": 8, \"Dynamic_Size\": False, 'Metod': 'tSNE', \"ValidRatio\": 0.1, \"seed\": 180,\n",
        "          \"dir\": \"/content/8/\", \"Mode\": \"CNN_Nature\",  # Mode : CNN_Nature, CNN2    | DIR: local working directory\n",
        "          \"res\":\"/content/drive/My Drive/dataset2/res/8/\",#remote path for saving data \n",
        "          \"LoadFromJson\": True, \"mutual_info\": False, # LoadFromJson: True: load pickle files (images)\n",
        "          #MOVE Ytrain.pickle and Ytest.pickle to the working directory after generating the images\n",
        "          \"hyper_opt_evals\": 50, \"epoch\": 40, \"No_0_MI\": False,  # True -> Removing 0 MI Features\n",
        "          \"autoencoder\": False\n",
        "          }\n",
        "\n",
        "\n",
        "\n",
        "if not param[\"LoadFromJson\"]:\n",
        "    data = {}\n",
        "    with open(param[\"dir\"] + 'Train.csv', 'r') as file:\n",
        "        data = {\"Xtrain\": pd.DataFrame(list(csv.DictReader(file))).astype(float), \"class\": 2}\n",
        "        data[\"Classification\"] = data[\"Xtrain\"][\"classification\"]\n",
        "        del data[\"Xtrain\"][\"classification\"]\n",
        "    with open(param[\"dir\"]+'Test_UNSW_NB15.csv', 'r') as file:\n",
        "        Xtest = pd.DataFrame(list(csv.DictReader(file)))\n",
        "        Xtest.replace(\"\", np.nan, inplace=True)\n",
        "        Xtest.dropna(inplace=True)\n",
        "        data[\"Xtest\"] = Xtest.astype(float)\n",
        "        data[\"Ytest\"] = data[\"Xtest\"][\"classification\"]\n",
        "        del data[\"Xtest\"][\"classification\"]\n",
        "\n",
        "     # AUTOENCODER\n",
        "    if param[\"autoencoder\"]:\n",
        "        autoencoder = load_model(param[\"dir\"] + 'Autoencoder.h5')\n",
        "        autoencoder.summary()\n",
        "        encoder = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('encod2').output)\n",
        "        encoder.summary()\n",
        "        # usa l'encoder con predict sul train_X e poi su test_X. Io qui ho creato anche il dataframe per salvarlo poi come csv\n",
        "        encoded_train = pd.DataFrame(encoder.predict(data[\"Xtrain\"]))\n",
        "        data[\"Xtrain\"] = encoded_train.add_prefix('feature_')\n",
        "        encoded_test = pd.DataFrame(encoder.predict(data[\"Xtest\"]))\n",
        "        data[\"Xtest\"] = encoded_test.add_prefix('feature_')\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'YTrain.pickle', 'wb')\n",
        "    pickle.dump(data[\"Classification\"], f_myfile)\n",
        "    f_myfile.close()\n",
        "    \n",
        "    f_myfile = open(param[\"dir\"] + 'YTest.pickle', 'wb')\n",
        "    pickle.dump(data[\"Ytest\"], f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    model = train_norm(param, data, norm=False)\n",
        "\n",
        "else:\n",
        "    images = {}\n",
        "    f_myfile = open(param[\"dir\"] + 'train_8x8_Mean.pickle', 'rb')\n",
        "    images[\"Xtrain\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'YTrain.pickle', 'rb')\n",
        "    images[\"Classification\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'test_8x8_Mean.pickle', 'rb')\n",
        "    images[\"Xtest\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open(param[\"dir\"] + 'YTest.pickle', 'rb')\n",
        "    images[\"Ytest\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    # with open('dataset/CICDS2017/Test.csv', 'r') as file:\n",
        "    #     Xtest=pd.DataFrame(list(csv.DictReader(file)))\n",
        "    #     Xtest.replace(\"\", np.nan, inplace=True)\n",
        "    #     Xtest.dropna(inplace=True)\n",
        "    #     images[\"Ytest\"] = Xtest[\"Classification\"]\n",
        "\n",
        "    model = train_norm(param, images, norm=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjwqBn5hQDXb",
        "colab_type": "code",
        "outputId": "90b62afe-e109-45cf-ec62-5154860fcdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import tensorflow as tf; print(tf.__version__)\n",
        "import tensorflow.keras; print(tensorflow.keras.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n",
            "2.3.0-tf\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}