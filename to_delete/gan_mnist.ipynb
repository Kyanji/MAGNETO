{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imageio in c:\\users\\deros\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\deros\\anaconda3\\lib\\site-packages (from imageio) (1.18.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\deros\\anaconda3\\lib\\site-packages (from imageio) (7.0.0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "low should be less than high: 0.01 is not smaller than 0.0001",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-1c3eabab3b2e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     33\u001B[0m optimizable_variable = {\"BATCH_SIZE\": hp.choice(\"BATCH_SIZE\", [64,128,256]),\n\u001B[0;32m     34\u001B[0m                         \u001B[1;34m'dropout_rate'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mhp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muniform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"dropout_rate\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m  \u001B[1;31m#Best PARAM = 0.3\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 35\u001B[1;33m                         \u001B[1;34m'lr_initial_g'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mhp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muniform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"lr_initial_g\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1e-2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1e-4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m   \u001B[1;31m# 1e-4\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     36\u001B[0m                         \u001B[1;31m#'lr_initial_d': hp.uniform(\"lr_initial_d\", 1e-2, 1e-4)   # 1e-4\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m                         \u001B[1;31m#\"lr_initial_d\": hp.uniform(\"lr_initial_d\", 0.001, 0.0001)    # 1e-4\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\pyll_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(label, *args, **kwargs)\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mis_real_string\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mis_literal_string\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"require string label\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\pyll_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(label, *args, **kwargs)\u001B[0m\n\u001B[0;32m     38\u001B[0m             raise ValueError(\n\u001B[0;32m     39\u001B[0m                 \u001B[1;34m\"low should be less than high: %s is not smaller than %s\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m                 \u001B[1;33m%\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mmin_val\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmax_val\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m             )\n\u001B[0;32m     42\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: low should be less than high: 0.01 is not smaller than 0.0001"
     ]
    }
   ],
   "source": [
    "#@title Default title text\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__\n",
    "\n",
    "# To generate GIFs\n",
    "!pip install imageio\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import tpe, hp, Trials, fmin\n",
    "from IPython import display\n",
    "from matplotlib import pyplot\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import tpe, hp, Trials, fmin\n",
    "from keras import backend as K\n",
    "\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 0\n",
    "optimizable_variable = {\"BATCH_SIZE\": hp.choice(\"BATCH_SIZE\", [64,128,256]),\n",
    "                        'dropout_rate': hp.uniform(\"dropout_rate\", 0, 1),  #Best PARAM = 0.3\n",
    "                        'lr_initial_g': hp.uniform(\"lr_initial_g\", 1e-2, 1e-4),   # 1e-4\n",
    "                        #'lr_initial_d': hp.uniform(\"lr_initial_d\", 1e-2, 1e-4)   # 1e-4\n",
    "                        #\"lr_initial_d\": hp.uniform(\"lr_initial_d\", 0.001, 0.0001)    # 1e-4\n",
    "                        }\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "#dropout_rate=0.3\n",
    "\n",
    "# Batch and shuffle the data\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "def make_generator_model(dropout_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(noise_dim,)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_discriminator_model(dropout_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss/2\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "\n",
    "def train_step(images,train_d,train_g,generator,discriminator,generator_optimizer,discriminator_optimizer):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    if train_d:\n",
    "      gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "      discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    if train_g:\n",
    "      gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "      generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "def plot_history(d_hist, g_hist, step=0, is_global=False):\n",
    "    # plot loss\n",
    "    pyplot.subplot(2, 1, 1)\n",
    "    pyplot.plot(d_hist, label='d')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "total_step=0\n",
    "\n",
    "def opt(param):\n",
    "  global BATCH_SIZE\n",
    "  BATCH_SIZE=param[\"BATCH_SIZE\"]\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(param[\"BATCH_SIZE\"])\n",
    "  generator = make_generator_model(param[\"dropout_rate\"])\n",
    "\n",
    "  noise = tf.random.normal([1, 100])\n",
    "  discriminator = make_discriminator_model(param[\"dropout_rate\"])\n",
    "\n",
    "\n",
    "  generator_optimizer = tf.keras.optimizers.Adam(param[\"lr_initial_g\"])\n",
    "  param[\"lr_initial_d\"]=param[\"lr_initial_g\"]\n",
    "  discriminator_optimizer = tf.keras.optimizers.Adam(param[\"lr_initial_d\"])\n",
    "\n",
    "\n",
    "  loss,generator,discriminator=train(train_dataset, EPOCHS,generator,discriminator,generator_optimizer,discriminator_optimizer,param)\n",
    "\n",
    "  noise = tf.random.normal([1024, 100])\n",
    "  predictions = generator(noise, training=False)\n",
    "  fig = plt.figure(figsize=(5,5))\n",
    "  global total_step\n",
    "  for i in range(25):\n",
    "      plt.subplot(5, 5, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig(\"results/img_\"+str(total_step)+\".png\")\n",
    "  plt.show()\n",
    "  #sottrazione media\n",
    "  errors =  - predictions\n",
    "  scalar_error=np.mean(np.mean(np.power(train_images[0:1024] - predictions, 2), axis=1))\n",
    "  print(scalar_error)\n",
    "  K.clear_session()\n",
    "  global res_error\n",
    "  res_error.append(param)\n",
    "  res_error[-1][\"error\"]=scalar_error\n",
    "  with open(\"results/resError.csv\", 'w', newline='') as csvfile:\n",
    "          writer = csv.DictWriter(csvfile, fieldnames=res[0].keys())\n",
    "          writer.writeheader()\n",
    "          writer.writerows(res)\n",
    "  return {'loss': scalar_error, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "\n",
    "res=[]\n",
    "res_error=[]\n",
    "def train(dataset, epochs,generator,discriminator,generator_optimizer,discriminator_optimizer,param):\n",
    "  print(param)\n",
    "  global res\n",
    "  global total_step\n",
    "  total_step=total_step+1\n",
    "  gen_ls=[]\n",
    "  disc_ls=[]\n",
    "  train_g=True\n",
    "  train_d=True\n",
    "  loss1=[]\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    gen=[]\n",
    "    disc=[]\n",
    "    for image_batch in dataset:\n",
    "      gen_loss, disc_loss= train_step(image_batch,train_d,train_g,generator,discriminator,generator_optimizer,discriminator_optimizer)\n",
    "      disc.append(disc_loss)\n",
    "      gen.append(gen_loss)\n",
    "\n",
    "    gen_ls.append(np.mean(gen))\n",
    "    disc_ls.append(np.mean(disc))\n",
    "    loss1.append(np.mean(gen)+np.mean(disc))\n",
    "    res.append(param)\n",
    "    res[-1].update({\"gen\":gen_ls[-1],\"disc\":disc_ls[-1]})\n",
    "    with open(\"results/res.csv\", 'w', newline='') as csvfile:\n",
    "          writer = csv.DictWriter(csvfile, fieldnames=res[0].keys())\n",
    "          writer.writeheader()\n",
    "          writer.writerows(res)\n",
    "    # Produce images for the GIF as we go\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "    plot_history(disc_ls,gen_ls)\n",
    "\n",
    "\n",
    "    if loss1[-1]==min(loss1):\n",
    "      print(\"new_best_model\")\n",
    "      generator.save_weights(\"gen_\"+str(total_step)+\"_\"+str(epoch)+\".h5\")\n",
    "      discriminator.save_weights(\"disc_\"+str(total_step)+\"_\"+str(epoch)+\".h5\")\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "    print(\"GEN:\"+str(train_g)+\" DISC\"+str(train_d))\n",
    "    print(\"GEN:\"+str( gen_ls[-1])+\" DISC\"+str( disc_ls[-1]))\n",
    "\n",
    "  # load best model\n",
    "  generator.load_weights(\"gen_\"+str(total_step)+\"_\"+str(np.argmin(loss1))+\".h5\")\n",
    "  discriminator.load_weights(\"disc_\"+str(total_step)+\"_\"+str(np.argmin(loss1))+\".h5\")\n",
    "  #result folder\n",
    "  generator.save_weights(\"results/gen_\"+str(total_step)+\"_\"+str(epoch)+\".h5\")\n",
    "  discriminator.save_weights(\"results/disc_\"+str(total_step)+\"_\"+str(epoch)+\".h5\")\n",
    "\n",
    "\n",
    "  return min(loss1),generator,discriminator\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "trials= Trials()\n",
    "print(optimizable_variable)\n",
    "newpath = r'results'\n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "fmin(opt, optimizable_variable, algo=tpe.suggest, max_evals=20)\n",
    "\n",
    "\n",
    "#train(train_dataset, EPOCHS)\n",
    "\n",
    "\"\"\"Restore the latest checkpoint.\"\"\"\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "\"\"\"## Create a GIF\"\"\"\n",
    "\n",
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "\n",
    "display_image(EPOCHS)\n",
    "\n",
    "\"\"\"Use `imageio` to create an animated gif using the images saved during training.\"\"\"\n",
    "\n",
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "  display.Image(filename=anim_file)\n",
    "\n",
    "\"\"\"If you're working in Colab you can download the animation with the code below:\"\"\"\n",
    "\n",
    "try:\n",
    "  from google.colab import files\n",
    "except ImportError:\n",
    "   pass\n",
    "else:\n",
    "  files.download(anim_file)\n",
    "\n",
    "\"\"\"## Next steps\n",
    "\n",
    "This tutorial has shown the complete code necessary to write and train a GAN. As a next step, you might like to experiment with a different dataset, for example the Large-scale Celeb Faces Attributes (CelebA) dataset [available on Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset). To learn more about GANs we recommend the [NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160).\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}